import json
import asyncio
import os
from typing import AsyncGenerator, List
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

from ..models.models import StreamingAgentState, SearchResult
from ...services.search.search_tools import vector_db_search
from ...services.search.search_tools import debug_web_search
from .orchestrator import OrchestratorAgent

# ì¶”ê°€: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
PERSONA_PROMPTS = {}
try:
    with open("agents/prompts/persona_prompts.json", "r", encoding="utf-8") as f:
        PERSONA_PROMPTS = json.load(f)
    print("SimpleAnswererAgent: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ.")
except Exception as e:
    print(f"SimpleAnswererAgent: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨ - {e}")


class SimpleAnswererAgent:
    """ë‹¨ìˆœ ì§ˆë¬¸ ì „ìš© Agent - ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì— ë§ê²Œ ìµœì í™”"""

    def __init__(self, model: str = "gemini-2.5-flash", temperature: float = 0.7):
        self.streaming_chat = ChatGoogleGenerativeAI(
            model=model, temperature=temperature
        )
        self.llm_lite = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash-lite", temperature=temperature
        )

        # OpenAI fallback ëª¨ë¸ë“¤
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if self.openai_api_key:
            self.llm_openai_mini = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=temperature,
                api_key=self.openai_api_key
            )
            print("SimpleAnswererAgent: OpenAI fallback ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            self.llm_openai_mini = None
            print("SimpleAnswererAgent: ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

        self.agent_type = "SIMPLE_ANSWERER"
        self.personas = PERSONA_PROMPTS

    async def _astream_with_fallback(self, prompt, primary_model, fallback_model):
        """
        ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ Gemini API rate limit ì‹œ OpenAIë¡œ fallback ì²˜ë¦¬
        """
        try:
            async for chunk in primary_model.astream(prompt):
                yield chunk
        except Exception as e:
            error_str = str(e).lower()
            rate_limit_indicators = ['429', 'quota', 'rate limit', 'exceeded', 'resource_exhausted']

            if any(indicator in error_str for indicator in rate_limit_indicators):
                print(f"SimpleAnswererAgent: Gemini API rate limit ê°ì§€, OpenAIë¡œ fallback ì‹œë„: {e}")
                if fallback_model:
                    try:
                        print("SimpleAnswererAgent: OpenAI fallbackìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
                        async for chunk in fallback_model.astream(prompt):
                            yield chunk
                    except Exception as fallback_error:
                        print(f"SimpleAnswererAgent: OpenAI fallbackë„ ì‹¤íŒ¨: {fallback_error}")
                        raise fallback_error
                else:
                    print("SimpleAnswererAgent: OpenAI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                    raise e
            else:
                raise e

    async def _invoke_with_fallback(self, prompt, primary_model, fallback_model):
        """
        Gemini API rate limit ì‹œ OpenAIë¡œ fallback ì²˜ë¦¬
        """
        try:
            result = await primary_model.ainvoke(prompt)
            return result
        except Exception as e:
            error_str = str(e).lower()
            rate_limit_indicators = ['429', 'quota', 'rate limit', 'exceeded', 'resource_exhausted']

            if any(indicator in error_str for indicator in rate_limit_indicators):
                print(f"SimpleAnswererAgent: Gemini API rate limit ê°ì§€, OpenAIë¡œ fallback ì‹œë„: {e}")
                if fallback_model:
                    try:
                        result = await fallback_model.ainvoke(prompt)
                        print("SimpleAnswererAgent: OpenAI fallback ì„±ê³µ")
                        return result
                    except Exception as fallback_error:
                        print(f"SimpleAnswererAgent: OpenAI fallbackë„ ì‹¤íŒ¨: {fallback_error}")
                        raise fallback_error
                else:
                    print("SimpleAnswererAgent: OpenAI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                    raise e
            else:
                raise e

    async def answer_streaming(self, state: StreamingAgentState) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        print("\n>> SimpleAnswerer: ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ì‹œì‘")

        query = state["original_query"]  # ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ ë°©ì‹ ì‚¬ìš©

        # --- ìˆ˜ì •: stateì— í˜ë¥´ì†Œë‚˜ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸ ---
        selected_persona = state.get("persona", "ê¸°ë³¸")
        
        # ì„ íƒëœ í˜ë¥´ì†Œë‚˜ê°€ ìœ íš¨í•œì§€ í™•ì¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •)
        if selected_persona not in self.personas:
            print(f" - ì•Œ ìˆ˜ ì—†ëŠ” í˜ë¥´ì†Œë‚˜ '{selected_persona}', 'ê¸°ë³¸'ìœ¼ë¡œ ì„¤ì •")
            selected_persona = "ê¸°ë³¸"
            state["persona"] = selected_persona
        
        print(f"  - ì±„íŒ…ì— '{selected_persona}' í˜ë¥´ì†Œë‚˜ ì ìš©")
        # ---------------------------------------------

        # ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (í•„ìš”ì‹œ)
        search_results = []
        need_web_search, web_search_query, need_vector_search, vector_search_query = await self._needs_search(query)

        print(f"- ê²€ìƒ‰ í•„ìš” ì—¬ë¶€: ì›¹={need_web_search}, ë²¡í„°={need_vector_search}")

        # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ë° ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
        if need_web_search:
            print(f"- ì›¹ ê²€ìƒ‰ í•„ìš”: {web_search_query}")
            web_results = await self._simple_web_search(web_search_query)
            if web_results:
                search_results.extend(web_results)
                # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë° (JSON ì´ë²¤íŠ¸ë¡œ)
                search_event = {
                    "type": "search_results",
                    "step": 1,
                    "tool_name": "web_search",
                    "query": web_search_query,
                    "results": [
                        {
                            "title": result.title,
                            "content_preview": result.content[:200] + "..." if len(result.content) > 200 else result.content,
                            "url": result.url if hasattr(result, 'url') else None,
                            "source": result.source,
                            "score": getattr(result, 'score', getattr(result, 'relevance_score', 0.9)),
                            "document_type": getattr(result, 'document_type', 'web')
                        }
                        for result in web_results
                    ],
                    # ğŸ†• Chat ëª¨ë“œ ê²€ìƒ‰ì„ì„ í‘œì‹œ
                    "is_intermediate_search": False,
                    "section_context": None,
                    "message_id": state.get("message_id")
                }
                yield json.dumps(search_event)
                print(f"- ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(web_results)}ê°œ ê²°ê³¼")

        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ ë° ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
        if need_vector_search:
            print(f"- ë²¡í„° ê²€ìƒ‰ í•„ìš”: {vector_search_query}")
            vector_results = await self._simple_vector_search(vector_search_query)
            if vector_results:
                search_results.extend(vector_results)
                # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë° (JSON ì´ë²¤íŠ¸ë¡œ)
                search_event = {
                    "type": "search_results",
                    "step": 2,
                    "tool_name": "vector_db_search",
                    "query": vector_search_query,
                    "results": [
                        {
                            "title": result.title,
                            "content_preview": result.content[:200] + "..." if len(result.content) > 200 else result.content,
                            "url": result.url if hasattr(result, 'url') else None,
                            "source": result.source,
                            "score": getattr(result, 'relevance_score', getattr(result, 'score', 0.7)),
                            "document_type": result.document_type
                        }
                        for result in vector_results
                    ],
                    # ğŸ†• Chat ëª¨ë“œ ê²€ìƒ‰ì„ì„ í‘œì‹œ
                    "is_intermediate_search": False,
                    "section_context": None,
                    "message_id": state.get("message_id")
                }
                yield json.dumps(search_event)
                print(f"- ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(vector_results)}ê°œ ê²°ê³¼")

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        memory_context = state.get("metadata", {}).get("memory_context", "")
        if memory_context:
            print(f"- ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©: {len(memory_context)}ì")

        full_response = ""
        prompt = self._create_enhanced_prompt_with_memory(
            query, search_results, state
        )

        try:
            chunk_count = 0
            content_generated = False

            async for chunk in self._astream_with_fallback(
                prompt,
                self.streaming_chat,
                self.llm_openai_mini
            ):
                chunk_count += 1
                if hasattr(chunk, 'content') and chunk.content:
                    content_generated = True
                    full_response += chunk.content
                    yield chunk.content
                    print(f">> SimpleAnswerer ì²­í¬ {chunk_count}: {len(chunk.content)} ë¬¸ì")

            print(f">> SimpleAnswerer ì™„ë£Œ: ì´ {chunk_count}ê°œ ì²­í¬, {len(full_response)} ë¬¸ì")

            # ë‚´ìš©ì´ ì „í˜€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° fallback ì²˜ë¦¬
            if not content_generated or not full_response.strip():
                print(">> ê²½ê³ : SimpleAnswererì—ì„œ ë‚´ìš©ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ, fallback ì‹¤í–‰")
                fallback_response = f"""ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì‚¬ìš©ì ì§ˆë¬¸**: {query}

ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜, ì ì‹œ í›„ì— ë‹¤ì‹œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."""
                yield fallback_response
                full_response = fallback_response

        except Exception as e:
            print(f"- LLM ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            # fallback ì‘ë‹µ
            fallback_response = f"""ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì‚¬ìš©ì ì§ˆë¬¸**: {query}

ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜, ì ì‹œ í›„ì— ë‹¤ì‹œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."""

            yield fallback_response
            full_response = fallback_response

        state["final_answer"] = full_response
        state["metadata"]["simple_answer_completed"] = True

        # ì¶œì²˜ ì •ë³´ ì €ì¥ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©)
        if search_results:
            sources_data = []
            full_data_dict = {}
            
            for idx, result in enumerate(search_results[:10]):  # ìµœëŒ€ 10ê°œë¡œ ì¦ê°€
                source_data = {
                    "id": idx + 1,
                    "title": getattr(result, 'metadata', {}).get("title", result.title or "ìë£Œ"),
                    "content": result.content[:300] + "..." if len(result.content) > 300 else result.content,
                    "url": result.url if hasattr(result, 'url') else None,
                    "source_url": result.source_url if hasattr(result, 'source_url') else None,
                    "source_type": result.source if hasattr(result, 'source') else "unknown"
                }
                sources_data.append(source_data)
                
                # full_data_dict ìƒì„± (0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤ ì‚¬ìš©)
                full_data_dict[idx] = {
                    "title": getattr(result, 'metadata', {}).get("title", result.title or "ìë£Œ"),
                    "content": result.content,
                    "source": result.source if hasattr(result, 'source') else "unknown",
                    "url": result.url if hasattr(result, 'url') else "",
                    "source_url": result.source_url if hasattr(result, 'source_url') else "",
                    "score": getattr(result, 'relevance_score', getattr(result, 'score', 0.0)),
                    "document_type": getattr(result, 'document_type', 'unknown')
                }
            
            state["metadata"]["sources"] = sources_data
            
            # full_data_dictë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡ (JSON ì´ë²¤íŠ¸ë¡œ)
            if full_data_dict:
                full_data_event = {
                    "type": "full_data_dict",
                    "data_dict": full_data_dict
                }
                yield json.dumps(full_data_event)
                print(f"- SimpleAnswerer full_data_dict ì „ì†¡ ì™„ë£Œ: {len(full_data_dict)}ê°œ í•­ëª©")

        print(f"- ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(full_response)}ì)")

    async def _simple_web_search(self, query: str) -> List[SearchResult]:
        """ê°„ë‹¨í•œ ì›¹ ê²€ìƒ‰"""
        try:
            result_text = await asyncio.get_event_loop().run_in_executor(
                None, debug_web_search, query
            )

            # ê²°ê³¼ê°€ ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
            search_results = []
            if result_text and isinstance(result_text, str):
                # ê°„ë‹¨í•œ íŒŒì‹±ìœ¼ë¡œ SearchResult ê°ì²´ ìƒì„±
                lines = result_text.split('\n')
                current_result = {}

                for line in lines:
                    line = line.strip()
                    if line.startswith(('1.', '2.', '3.', '4.', '5.')):
                        # ì´ì „ ê²°ê³¼ ì €ì¥
                        if current_result:
                            search_result = SearchResult(
                                source="web_search",
                                content=current_result.get("snippet", ""),
                                search_query=query,
                                title=current_result.get("title", "ì›¹ ê²€ìƒ‰ ê²°ê³¼"),
                                url=current_result.get("link"),
                                relevance_score=0.9,  # ì›¹ê²€ìƒ‰ ê²°ê³¼ëŠ” ë†’ì€ ì ìˆ˜
                                timestamp=datetime.now().isoformat(),
                                document_type="web",
                                metadata={"original_query": query, **current_result},
                                source_url=current_result.get("link", "ì›¹ ê²€ìƒ‰ ê²°ê³¼")
                            )
                            search_results.append(search_result)

                        # ìƒˆ ê²°ê³¼ ì‹œì‘
                        current_result = {"title": line[3:].strip()}  # ë²ˆí˜¸ ì œê±°
                    elif line.startswith("ì¶œì²˜ ë§í¬:"):
                        current_result["link"] = line[7:].strip()  # "ì¶œì²˜ ë§í¬:" ì œê±°
                    elif line.startswith("ìš”ì•½:"):
                        current_result["snippet"] = line[3:].strip()

                # ë§ˆì§€ë§‰ ê²°ê³¼ ì €ì¥
                if current_result:
                    search_result = SearchResult(
                        source="web_search",
                        content=current_result.get("snippet", ""),
                        search_query=query,
                        title=current_result.get("title", "ì›¹ ê²€ìƒ‰ ê²°ê³¼"),
                        url=current_result.get("link"),
                        relevance_score=0.9,
                        timestamp=datetime.now().isoformat(),
                        document_type="web",
                        metadata={"original_query": query, **current_result},
                        source_url=current_result.get("link", "ì›¹ ê²€ìƒ‰ ê²°ê³¼")
                    )
                    search_results.append(search_result)

            print(f"- ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results[:3]  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ
        except Exception as e:
            print(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _simple_vector_search(self, query: str) -> List[SearchResult]:
        """ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰"""
        try:
            results = await asyncio.get_event_loop().run_in_executor(
                None, vector_db_search, query
            )

            search_results = []
            for result in results[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if isinstance(result, dict):
                    search_result = SearchResult(
                        source="vector_db",
                        content=result.get("content", ""),
                        search_query=query,
                        title=result.get("title", "ë²¡í„° DB ë¬¸ì„œ"),
                        url=None,
                        relevance_score=result.get("similarity_score", 0.7),
                        timestamp=datetime.now().isoformat(),
                        document_type="database",
                        similarity_score=result.get("similarity_score", 0.7),
                        metadata=result
                    )
                    search_results.append(search_result)

            return search_results
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _needs_search(self, query: str):
        """ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ ì—¬ë¶€ë¥¼ íŒë‹¨"""
        try:
            prompt = f"""
ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.
ì§ˆë¬¸: {query}
ì˜¤ëŠ˜ ë‚ ì§œ : {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}
Web ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ True, ì•„ë‹ˆë©´ Falseë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
Vector DB ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ True, ì•„ë‹ˆë©´ Falseë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
- Web ê²€ìƒ‰ì€ ìµœê·¼ ì •ë³´, ì´ìŠˆ, ê°„ë‹¨í•œ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©
- Vector DB ê²€ìƒ‰ì€ íŠ¹ì • ë°ì´í„°, ë¬¸ì„œ, í˜„í™©, í†µê³„, ë‚´ë¶€ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©

ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œ/í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
            "needs_web_search": false,
            "web_search_query": "ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬",
            "needs_vector_search": false,
            "vector_search_query": "ë²¡í„° DB ê²€ìƒ‰ ì¿¼ë¦¬"
}}

ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ì˜ˆì‹œ
- "2025ë…„ ìµœì‹  ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ íŠ¸ë Œë“œ"
ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ ì˜ˆì‹œ
- "2025ë…„ ìœ í–‰í•˜ëŠ” ê±´ê°•ì‹í’ˆì´ ë­ê°€ ìˆë‚˜ìš”?"

ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ìœ¼ë¡œ
ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” ì§ˆë¬¸í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
        """
            response = await self._invoke_with_fallback(
                prompt,
                self.llm_lite,
                self.llm_openai_mini
            )
            response_content = response.content.strip()

            # JSON íŒŒì‹± ì‹œë„ - ê°œì„ ëœ íŒŒì‹± ë¡œì§
            try:
                # ì½”ë“œ ë¸”ë¡ ì œê±°
                clean_response = response_content
                if "```json" in response_content:
                    clean_response = response_content.split("```json")[1].split("```")[0].strip()
                elif "```" in response_content:
                    clean_response = response_content.split("```")[1].split("```")[0].strip()

                # JSON íŒŒì‹±
                response_json = json.loads(clean_response)
                needs_web_search = response_json.get("needs_web_search", False)
                web_search_query = response_json.get("web_search_query", "")
                needs_vector_search = response_json.get("needs_vector_search", False)
                vector_search_query = response_json.get("vector_search_query", "")

                print(f"- ê²€ìƒ‰ íŒë‹¨ ì™„ë£Œ: ì›¹={needs_web_search}, ë²¡í„°={needs_vector_search}")
                return needs_web_search, web_search_query, needs_vector_search, vector_search_query

            except json.JSONDecodeError as e:
                print(f"- JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                print(f"- LLM ì‘ë‹µ: {response_content[:200]}...")

                # ë¬¸ìì—´ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ fallback íŒŒï¿½ing
                needs_web_search = False
                needs_vector_search = False

                # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
                if "needs_web_search" in response_content:
                    if "needs_web_search\": true" in response_content or "needs_web_search\":true" in response_content:
                        needs_web_search = True

                if "needs_vector_search" in response_content:
                    if "needs_vector_search\": true" in response_content or "needs_vector_search\":true" in response_content:
                        needs_vector_search = True

                print(f"- Fallback íŒŒì‹± ê²°ê³¼: ì›¹={needs_web_search}, ë²¡í„°={needs_vector_search}")
                # ê¸°ë³¸ê°’ ë°˜í™˜ (ê°„ë‹¨í•œ ì¸ì‚¬ëŠ” ê²€ìƒ‰ ë¶ˆí•„ìš”)
                return needs_web_search, "", needs_vector_search, ""

        except Exception as e:
            print(f"- _needs_search ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return False, "", False, ""


    def _create_enhanced_prompt_with_memory(
        self, query: str, search_results: List[SearchResult], state: StreamingAgentState
    ) -> str:
        """í˜ë¥´ì†Œë‚˜, ë©”ëª¨ë¦¬, ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        current_date_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

        # stateì—ì„œ í˜ë¥´ì†Œë‚˜ì™€ ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ì¶œ
        persona_name = state.get("persona", "ê¸°ë³¸")
        persona_instruction = self.personas.get(persona_name, {}).get("prompt", "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
        memory_context = state.get("metadata", {}).get("memory_context", "")

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        context_summary = ""
        if search_results:
            summary_parts = []
            for i, result in enumerate(search_results[:3]):
                content = result.content
                title = getattr(result, 'metadata', {}).get("title", result.title or "ìë£Œ")

                # URL ì •ë³´ ì¶”ê°€ (ì›¹ ê²€ìƒ‰ ê²°ê³¼ì¸ ê²½ìš°)
                url_info = ""
                if hasattr(result, 'url') and result.url:
                    url_info = f"\n  **ì¶œì²˜ ë§í¬**: {result.url}"
                elif hasattr(result, 'source_url') and result.source_url and not result.source_url.startswith(('ì›¹ ê²€ìƒ‰', 'Vector DB')):
                    url_info = f"\n  **ì¶œì²˜ ë§í¬**: {result.source_url}"

                summary_parts.append(f"**[ì°¸ê³ ìë£Œ {i}]** **{title}**: {content[:200]}...{url_info}")
            context_summary = "\n\n".join(summary_parts)

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
        memory_info = f"\n## ì´ì „ ëŒ€í™”\n{memory_context[:500]}...\n" if memory_context else ""

        return f"""{persona_instruction}

ìœ„ì˜ ë‹¹ì‹ ì˜ ì—­í• ê³¼ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€í‚¤ë©´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

í˜„ì¬ ë‚ ì§œ: {current_date_str}

{memory_info}

## ì°¸ê³  ì •ë³´
{context_summary if context_summary else "ì¶”ê°€ ì°¸ê³  ì •ë³´ ì—†ìŒ"}

## ì‚¬ìš©ì ì§ˆë¬¸
{query}

## ì‘ë‹µ ê°€ì´ë“œ
- **í˜ë¥´ì†Œë‚˜ ìœ ì§€**: ë‹¹ì‹ ì˜ ì—­í• ì— ë§ëŠ” ë§íˆ¬ì™€ ê´€ì ì„ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
- ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ë‹µë³€
- ì°¸ê³  ì •ë³´ê°€ ìˆìœ¼ë©´ ì´ë¥¼ í™œìš©í•˜ë˜, ì •í™•í•œ ì •ë³´ë§Œ ì‚¬ìš©
- ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„
- ê°„ê²°í•˜ë©´ì„œë„ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ ì œê³µ
- í•„ìš”ì‹œ ì¶”ê°€ ì§ˆë¬¸ì„ ê¶Œìœ 
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ ì‘ì„±
- ë§ˆí¬ë‹¤ìš´ì˜ '-', '*', '+', '##', '###' ë“±ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ì€ ë‹µë³€ ì‘ì„±
- **ì¤‘ìš”**: ì°¸ê³  ì •ë³´ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì„¸ìš”:
  * ë¬¸ì¥ ëì— [SOURCE:ìˆ«ì] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ ë²ˆí˜¸ë¥¼ í‘œê¸° (ìˆ«ìë§Œ ì‚¬ìš©, "ë°ì´í„°"ë‚˜ "ë¬¸ì„œ" ë“±ì˜ ë‹¨ì–´ ì‚¬ìš© ê¸ˆì§€)
  * ì˜ˆì‹œ: "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ ê·œëª¨ëŠ” 6ì¡° 440ì–µ ì›ì…ë‹ˆë‹¤ [SOURCE:0]"
  * ì˜ˆì‹œ: "ê²½ìŸì‚¬ì˜ ê²½ìš° ë°”ì´ëŸ´ì„ í†µí•œ ë§ˆì¼€íŒ… ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤ [SOURCE:1]"
  * ì˜ëª»ëœ ì˜ˆì‹œ: [SOURCE:ë°ì´í„° 1], [SOURCE:ë¬¸ì„œ 1] (ì´ëŸ° í˜•ì‹ ì‚¬ìš© ê¸ˆì§€)
  * ì°¸ê³  ì •ë³´ì˜ ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ 0, 1, 2... ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”

ë‹µë³€:"""
