# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import asyncio
import json
import os
import random
import re
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import Any, AsyncGenerator, Dict, List, Optional, Set

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests

# LangChain ê´€ë ¨
from langchain import hub
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

import json
from typing import Dict, List, AsyncGenerator
from langchain_openai import ChatOpenAI

# ë¡œì»¬ imports
from ...core.config.report_config import TeamType, ReportType, Language
from ...services.templates.report_templates import ReportTemplateManager
from ...services.builders.prompt_builder import PromptBuilder
from ...utils.analyzers.query_analyzer import QueryAnalyzer

# ë¡œì»¬ ëª¨ë“ˆ
from ..models.models import (
    AgentMessage,
    AgentType,
    ComplexityLevel,
    CriticResult,
    DatabaseType,
    ExecutionStrategy,
    MessageType,
    QueryPlan,
    SearchResult,
    StreamingAgentState,
)
from ...services.search.search_tools import (
    debug_web_search,
    graph_db_search,
    rdb_search,
    mock_vector_search,
)
from ...utils.utils import create_agent_message

class DataExtractor:
    """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‹¤ì œ ìˆ˜ì¹˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    async def extract_numerical_data(self, search_results: List[SearchResult], query: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìˆ˜ì¹˜ ë°ì´í„°ë¥¼ ì¶”ì¶œ"""


        combined_text = ""
        for result in search_results:
            combined_text += f"{result.content}\n"

        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì, í¼ì„¼íŠ¸, í†µê³„ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  JSON í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

        ì›ë³¸ ì§ˆë¬¸: {query}

        í…ìŠ¤íŠ¸:
        {combined_text[:]}  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        {{
            "extracted_numbers": [
                {{"value": ìˆ«ì, "unit": "ë‹¨ìœ„", "context": "ì„¤ëª…", "source": "ì¶œì²˜"}}
            ],
            "percentages": [
                {{"value": ìˆ«ì, "context": "ì„¤ëª…"}}
            ],
            "trends": [
                {{"period": "ê¸°ê°„", "change": "ë³€í™”ìœ¨", "description": "ì„¤ëª…"}}
            ],
            "categories": {{
                "category_name": {{"value": ìˆ«ì, "description": "ì„¤ëª…"}}
            }}
        }}

        ì‹¤ì œ ìˆ«ìê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ì´ë‚˜ ê°ì²´ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
        """

        try:
            response = await self.llm.ainvoke(prompt)
            return json.loads(response.content)
        except:
            return {"extracted_numbers": [], "percentages": [], "trends": [], "categories": {}}



class PlanningAgent:
    """
    4ë‹¨ê³„ ë³µì¡ë„ ë¶„ë¥˜ë¥¼ ì§€ì›í•˜ëŠ” í–¥ìƒëœ ê³„íš ìˆ˜ë¦½ ì—ì´ì „íŠ¸
    - SIMPLE: ì§ì ‘ ë‹µë³€ ê°€ëŠ¥
    - MEDIUM: ê¸°ë³¸ ê²€ìƒ‰ + ê°„ë‹¨ ë¶„ì„
    - COMPLEX: í’€ ReAct ì—ì´ì „íŠ¸ í™œìš©
    - SUPER_COMPLEX: ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—…
    """

    def __init__(self):
        self.chat = ChatOpenAI(model="gpt-4o", temperature=0)
        self.agent_type = AgentType.PLANNING

    async def plan(self, state: StreamingAgentState) -> StreamingAgentState:
        """ì§ˆë¬¸ì„ 4ë‹¨ê³„ë¡œ ë¶„ì„í•˜ê³  ìµœì ì˜ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½"""
        print(">> PLANNING ë‹¨ê³„ ì‹œì‘ (4ë‹¨ê³„ ë³µì¡ë„ ë¶„ë¥˜)")
        query = state.original_query
        print(f"- ì›ë³¸ ì¿¼ë¦¬: {query}")

        # ì´ì „ ë‹¨ê³„ í”¼ë“œë°± ìˆ˜ì§‘
        feedback_context = self._collect_feedback(state)
        if feedback_context:
            print(f"- í”¼ë“œë°± ë°˜ì˜: {feedback_context}")

        # 4ë‹¨ê³„ ë³µì¡ë„ ë¶„ì„
        complexity_analysis = await self._analyze_query_complexity_4levels(
            query, feedback_context
        )
        print(f"- ë³µì¡ë„ ë¶„ì„ ê²°ê³¼: {complexity_analysis}")

        # ë³µì¡ë„ë³„ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
        execution_plan = await self._create_execution_plan_by_complexity(
            query, complexity_analysis, feedback_context
        )

        # ë³µì¡ë„ì™€ ì „ëµ ê°’ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
        complexity_mapping = {
            "SIMPLE": "simple",
            "MEDIUM": "medium",
            "COMPLEX": "complex",
            "SUPER_COMPLEX": "super_complex",
            "simple": "simple",
            "medium": "medium",
            "complex": "complex",
            "super_complex": "super_complex",
        }

        strategy_mapping = {
            "direct_answer": "direct_answer",
            "basic_search": "basic_search",
            "full_react": "full_react",
            "multi_agent": "multi_agent",
        }

        # ì›ë³¸ ê°’ë“¤
        raw_complexity = complexity_analysis.get("complexity_level", "MEDIUM")
        raw_strategy = complexity_analysis.get("execution_strategy", "basic_search")

        # ë§¤í•‘ëœ ê°’ë“¤
        mapped_complexity = complexity_mapping.get(raw_complexity, "medium")
        mapped_strategy = strategy_mapping.get(raw_strategy, "basic_search")

        print(f"- ë³µì¡ë„ ë§¤í•‘: {raw_complexity} â†’ {mapped_complexity}")
        print(f"- ì „ëµ ë§¤í•‘: {raw_strategy} â†’ {mapped_strategy}")

        state.query_plan = QueryPlan(
            original_query=query,
            sub_queries=[execution_plan],
            estimated_complexity=mapped_complexity,  # ë§¤í•‘ëœ ì†Œë¬¸ì ê°’ ì‚¬ìš©
            execution_strategy=mapped_strategy,  # ë§¤í•‘ëœ ì†Œë¬¸ì ê°’ ì‚¬ìš©
            resource_requirements=complexity_analysis.get("resource_requirements", {}),
        )

        state.planning_complete = True
        print(f">> PLANNING ë‹¨ê³„ ì™„ë£Œ - ë³µì¡ë„: {mapped_complexity}")
        return state

    def _collect_feedback(self, state: StreamingAgentState) -> Optional[str]:
        """ì´ì „ ë‹¨ê³„ì˜ í”¼ë“œë°±ì„ ìˆ˜ì§‘"""
        if state.critic2_result and state.critic2_result.status == "insufficient":
            return f"ìµœì¢… ê²€ìˆ˜ í”¼ë“œë°±: {state.critic2_result.suggestion}"
        elif state.critic1_result and state.critic1_result.status == "insufficient":
            return f"ì´ˆê¸° ìˆ˜ì§‘ í”¼ë“œë°±: {state.critic1_result.suggestion}"
        return None

    async def _analyze_query_complexity_4levels(
        self, query: str, feedback: Optional[str] = None
    ) -> Dict:
        """ì§ˆë¬¸ì„ 4ë‹¨ê³„ ë³µì¡ë„ë¡œ ë¶„ì„"""

        feedback_section = ""
        if feedback:
            feedback_section = f"""
## ì´ì „ ì‹œë„ í”¼ë“œë°±
{feedback}

ìœ„ í”¼ë“œë°±ì„ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""

        prompt = f"""ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ AI ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 4ë‹¨ê³„ ë³µì¡ë„ë¡œ ì •í™•íˆ ë¶„ë¥˜í•´ì•¼ í•©ë‹ˆë‹¤.

{feedback_section}

## ë¶„ì„ ëŒ€ìƒ ì§ˆë¬¸
"{query}"

## 4ë‹¨ê³„ ë³µì¡ë„ ë¶„ë¥˜ ê¸°ì¤€

### SIMPLE (ì§ì ‘ ë‹µë³€)
- ë‹¨ì¼ ì •ë³´ ìš”ì²­, ê¸°ë³¸ ì •ì˜, ê°„ë‹¨í•œ ê³„ì‚°
- ì¶”ê°€ ê²€ìƒ‰ì´ë‚˜ ë¶„ì„ ë¶ˆí•„ìš”
- 1-2ê°œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥
- ì˜ˆ: "ì•„ë§ˆë€ìŠ¤ê°€ ë­ì•¼?", "ì¹¼ë¡œë¦¬ ì•Œë ¤ì¤˜"

### MEDIUM (ê¸°ë³¸ ê²€ìƒ‰ + ê°„ë‹¨ ë¶„ì„)
- ìµœì‹  ì •ë³´ë‚˜ ê°„ë‹¨í•œ ë¹„êµê°€ í•„ìš”
- 1-2ê°œ ì†ŒìŠ¤ì—ì„œ ì •ë³´ ìˆ˜ì§‘ í›„ ì¢…í•©
- ë‹¨ìˆœí•œ ë¶„ì„ì´ë‚˜ ìš”ì•½ í•„ìš”
- ì˜ˆ: "ì˜¤ëŠ˜ ì±„ì†Œ ì‹œì„¸ëŠ”?", "Aì™€ B ì°¨ì´ì ì€?"

### COMPLEX (í’€ ReAct ì—ì´ì „íŠ¸)
- ë‹¤ë‹¨ê³„ ì¶”ë¡ ê³¼ ì—¬ëŸ¬ ì†ŒìŠ¤ ì¢…í•© í•„ìš”
- ì „ëµì  ì‚¬ê³ ì™€ ë§¥ë½ì  ë¶„ì„ í•„ìš”
- ë³µì¡í•œ ì˜ì‚¬ê²°ì • ì§€ì›
- ì˜ˆ: "ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½í•´ì¤˜", "ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ"

### SUPER_COMPLEX (ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—…)
- ë§¤ìš° ë³µì¡í•œ ë‹¤ì˜ì—­ ë¶„ì„
- ì¥ê¸°ì  ê³„íšì´ë‚˜ ì¢…í•©ì  ì „ëµ í•„ìš”
- ì—¬ëŸ¬ ì „ë¬¸ê°€ ê´€ì  ì¢…í•© í•„ìš”
- ì˜ˆ: "ê¸€ë¡œë²Œ ì§„ì¶œ ì „ëµ", "5ë…„ ì‚¬ì—… ê³„íš"

## ì‹¤í–‰ ì „ëµ ë§¤í•‘

### SIMPLE â†’ "direct_answer"
- SimpleAnswererAgentë§Œ ì‚¬ìš©
- ì¦‰ì‹œ ë‹µë³€ ìƒì„±

### MEDIUM â†’ "basic_search"
- ê¸°ë³¸ Vector/RDB ê²€ìƒ‰
- ê°„ë‹¨í•œ LLM ë¶„ì„
- ReAct ì—†ì´ ì§ì ‘ ì²˜ë¦¬

### COMPLEX â†’ "full_react"
- ì™„ì „í•œ ReAct ì—ì´ì „íŠ¸ í™œìš©
- ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ë° ì¶”ë¡ 
- ìƒì„¸í•œ ë¶„ì„ ë° ì¢…í•©

### SUPER_COMPLEX â†’ "multi_agent"
- ì—¬ëŸ¬ ì „ë¬¸ ì—ì´ì „íŠ¸ í˜‘ì—…
- ë‹¨ê³„ë³„ ê²€ì¦ ë° í”¼ë“œë°±
- ì¢…í•©ì  ë³´ê³ ì„œ ìƒì„±

## ì¶œë ¥ í˜•ì‹
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

```json
{{
  "complexity_level": "SIMPLE|MEDIUM|COMPLEX|SUPER_COMPLEX",
  "execution_strategy": "direct_answer|basic_search|full_react|multi_agent",
  "reasoning": "íŒë‹¨ ê·¼ê±°ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…",
  "resource_requirements": {{
    "search_needed": true|false,
    "react_needed": true|false,
    "multi_agent_needed": true|false,
    "estimated_time": "fast|medium|slow|very_slow"
  }},
  "expected_output_type": "simple_text|analysis|report|comprehensive_strategy"
}}
```

ì •í™•í•œ JSON í˜•ì‹ì„ ì¤€ìˆ˜í•˜ê³ , ì§ˆë¬¸ì˜ ì§„ì§œ ë³µì¡ë„ë¥¼ ì‹ ì¤‘íˆ íŒë‹¨í•˜ì„¸ìš”."""

        try:
            response = await self.chat.ainvoke(prompt)
            # JSON íŒŒì‹± ì‹œë„
            import json

            # ì½”ë“œ ë¸”ë¡ì´ ìˆë‹¤ë©´ ì œê±°
            content = response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)
            return result

        except Exception as e:
            print(f"ë³µì¡ë„ ë¶„ì„ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {
                "complexity_level": "MEDIUM",
                "execution_strategy": "basic_search",
                "reasoning": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê¸°ë³¸ê°’ ì ìš©",
                "resource_requirements": {
                    "search_needed": True,
                    "react_needed": False,
                    "multi_agent_needed": False,
                    "estimated_time": "medium",
                },
                "expected_output_type": "analysis",
            }

    async def _create_execution_plan_by_complexity(
        self, query: str, complexity_analysis: Dict, feedback: Optional[str] = None
    ) -> str:
        """ë³µì¡ë„ë³„ ë§ì¶¤ ì‹¤í–‰ ê³„íš ìƒì„±"""

        complexity_level = complexity_analysis["complexity_level"]
        execution_strategy = complexity_analysis["execution_strategy"]

        if complexity_level == "SIMPLE":
            return f"ì§ì ‘ ë‹µë³€ ìƒì„±: {query}"

        elif complexity_level == "MEDIUM":
            return f"ê¸°ë³¸ ê²€ìƒ‰ í›„ ë¶„ì„: {query} - Vector DB ë° RDB ê²€ìƒ‰ í™œìš©"

        elif complexity_level == "COMPLEX":
            return await self._create_complex_execution_plan(
                query, complexity_analysis, feedback
            )

        elif complexity_level == "SUPER_COMPLEX":
            return await self._create_super_complex_execution_plan(
                query, complexity_analysis, feedback
            )

        else:
            return f"ê¸°ë³¸ ì‹¤í–‰ ê³„íš: {query}"

    async def _create_complex_execution_plan(
        self, query: str, analysis: Dict, feedback: Optional[str] = None
    ) -> str:
        """COMPLEX ë ˆë²¨ ì‹¤í–‰ ê³„íš - ê¸°ì¡´ ToT ë°©ì‹ í™œìš©"""

        feedback_section = ""
        if feedback:
            feedback_section = f"""
## ì¤‘ìš”: ì´ì „ ì‹œë„ í”¼ë“œë°±
{feedback}

ìœ„ í”¼ë“œë°±ì„ ë°˜ë“œì‹œ í•´ê²°í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
"""

        prompt = f"""ë‹¹ì‹ ì€ ì „ëµ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ë³µí•©ì  ë¶„ì„ì´ í•„ìš”í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì²´ê³„ì  ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

{feedback_section}

## í´ë¼ì´ì–¸íŠ¸ ìš”ì²­
"{query}"

## ìš”ì²­ ë¶„ì„ ê²°ê³¼
- ë³µì¡ë„: {analysis.get('complexity_level', 'COMPLEX')}
- ì˜ˆìƒ ê²°ê³¼ë¬¼: {analysis.get('expected_output_type', 'analysis')}
- íŒë‹¨ ê·¼ê±°: {analysis.get('reasoning', '')}

## ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì§€ì¹¨

ë‹¤ìŒ ë‹¨ê³„ë¡œ ì²´ê³„ì ì¸ ì‹¤í–‰ ê³„íšì„ ì‘ì„±í•˜ì„¸ìš”:

1. **ì •ë³´ ìˆ˜ì§‘ ì „ëµ**: ì–´ë–¤ ì •ë³´ë¥¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘í• ì§€
2. **ë¶„ì„ ì ‘ê·¼ë²•**: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ë¶„ì„í•˜ê³  ì¢…í•©í• ì§€
3. **ê²°ê³¼ë¬¼ êµ¬ì„±**: ìµœì¢… ë‹µë³€ì„ ì–´ë–¤ í˜•íƒœë¡œ ì œê³µí• ì§€

## ì¶œë ¥ í˜•ì‹
êµ¬ì²´ì ì¸ ì‹¤í–‰ ì§€ì‹œë¬¸ í˜•íƒœë¡œ ì‘ì„± (200-300ì):

**ì‹¤í–‰ ê³„íš:**
[ReAct ì—ì´ì „íŠ¸ê°€ ê·¸ëŒ€ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ê³„íš]
"""

        try:
            response = await self.chat.ainvoke(prompt)
            return response.content.strip()
        except Exception as e:
            print(f"ë³µí•© ê³„íš ìƒì„± ì˜¤ë¥˜: {e}")
            return f"'{query}' ìš”ì²­ì— ëŒ€í•œ ì²´ê³„ì  ë¶„ì„ê³¼ ì „ëµì  ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤."

    async def _create_super_complex_execution_plan(
        self, query: str, analysis: Dict, feedback: Optional[str] = None
    ) -> str:
        """SUPER_COMPLEX ë ˆë²¨ ì‹¤í–‰ ê³„íš - ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—…"""

        prompt = f"""ë‹¹ì‹ ì€ McKinsey ì‹œë‹ˆì–´ íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤. ë§¤ìš° ë³µì¡í•œ ì „ëµì  ê³¼ì œì— ëŒ€í•œ ë‹¤ë‹¨ê³„ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

## ì „ëµì  ê³¼ì œ
"{query}"

## ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… ê³„íš

ë‹¤ìŒ ê´€ì ì—ì„œ ì¢…í•©ì  ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:

1. **1ë‹¨ê³„ - í˜„í™© ë¶„ì„**: ì‹œì¥/ìƒí™© ë¶„ì„ ì „ë¬¸ê°€ ê´€ì 
2. **2ë‹¨ê³„ - ì „ëµ ìˆ˜ë¦½**: ì „ëµ ê¸°íš ì „ë¬¸ê°€ ê´€ì 
3. **3ë‹¨ê³„ - ì‹¤í–‰ ë°©ì•ˆ**: ì‹¤í–‰ ì „ë¬¸ê°€ ê´€ì 
4. **4ë‹¨ê³„ - ë¦¬ìŠ¤í¬ ê´€ë¦¬**: ìœ„í—˜ ê´€ë¦¬ ì „ë¬¸ê°€ ê´€ì 
5. **5ë‹¨ê³„ - ì¢…í•© ê²€ì¦**: í†µí•© ê²€ì¦ ë° ìµœì¢… ì œì•ˆ

ê° ë‹¨ê³„ë³„ë¡œ ì–´ë–¤ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ì–´ë–»ê²Œ ë¶„ì„í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.

**ë‹¤ë‹¨ê³„ ì‹¤í–‰ ê³„íš (300-400ì):**
"""

        try:
            response = await self.chat.ainvoke(prompt)
            return response.content.strip()
        except Exception as e:
            print(f"ì´ˆë³µí•© ê³„íš ìƒì„± ì˜¤ë¥˜: {e}")
            return f"'{query}' ìš”ì²­ì— ëŒ€í•œ ë‹¤ë‹¨ê³„ í˜‘ì—… ë¶„ì„ê³¼ ì¢…í•©ì  ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤."



class RetrieverAgent:
    """í†µí•© ê²€ìƒ‰ ì—ì´ì „íŠ¸ - ë³µì¡ë„ë³„ ì°¨ë“± + ë³‘ë ¬ ì²˜ë¦¬"""

    def __init__(self, vector_db=None, rdb=None, graph_db=None):
        self.vector_db = vector_db
        self.rdb = rdb
        self.graph_db = graph_db

        # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤
        self.available_tools = [
            debug_web_search,
            mock_vector_search,
            rdb_search,
            graph_db_search,
        ]

        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.chat = ChatOpenAI(model="gpt-3.5-turbo")

        # ë‚ ì§œ ì •ë³´
        self.current_date = datetime.now()
        self.current_date_str = self.current_date.strftime("%Yë…„ %mì›” %dì¼")
        self.current_year = self.current_date.year

        # ReAct ì—ì´ì „íŠ¸ (ë³µì¡í•œ ì¿¼ë¦¬ìš©)
        self.react_agent_executor = self._create_react_agent()

        # ë³‘ë ¬ ì²˜ë¦¬ìš© ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=4)

    async def search(self, state: StreamingAgentState) -> StreamingAgentState:
        """ë³µì¡ë„ë³„ ì°¨ë“± + ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰"""
        print(">> í†µí•© RETRIEVER ì‹œì‘")

        if not state.query_plan or not state.query_plan.sub_queries:
            print("- ì²˜ë¦¬í•  ì¿¼ë¦¬ê°€ ì—†ì–´ RETRIEVERë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return state

        # ë³µì¡ë„ ë° ì‹¤í–‰ ì „ëµ ê²°ì •
        complexity_level = state.get_complexity_level()
        execution_strategy = self._determine_execution_strategy(state, complexity_level)

        original_query = state.original_query

        print(f"- ë³µì¡ë„: {complexity_level}")
        print(f"- ì‹¤í–‰ ì „ëµ: {execution_strategy}")
        print(f"- ì›ë³¸ ì¿¼ë¦¬: {original_query}")

        # ì‹¤í–‰ ì „ëµì— ë”°ë¥¸ ë³‘ë ¬ ê²€ìƒ‰
        if execution_strategy == ExecutionStrategy.DIRECT_ANSWER:
            print("- SIMPLE: ê²€ìƒ‰ ìƒëµ")
            return state

        elif execution_strategy == ExecutionStrategy.BASIC_SEARCH:
            print("- BASIC: ê¸°ë³¸ ë³‘ë ¬ ê²€ìƒ‰")
            return await self._execute_basic_parallel_search(state, original_query)

        elif execution_strategy == ExecutionStrategy.FULL_REACT:
            print("- COMPLEX: í’€ ë³‘ë ¬ ê²€ìƒ‰ + ReAct")
            return await self._execute_full_parallel_search(state, original_query)

        elif execution_strategy == ExecutionStrategy.MULTI_AGENT:
            print("- SUPER_COMPLEX: ë‹¤ë‹¨ê³„ ë³‘ë ¬ ê²€ìƒ‰")
            return await self._execute_multi_stage_parallel_search(state, original_query)

        else:
            return await self._execute_basic_parallel_search(state, original_query)

    async def _execute_basic_parallel_search(
        self, state: StreamingAgentState, query: str
    ) -> StreamingAgentState:
        """ê¸°ë³¸ ë³‘ë ¬ ê²€ìƒ‰ (BASIC ë³µì¡ë„)"""
        print("\n>> ê¸°ë³¸ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰")

        # ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ê²€ìƒ‰ ì‘ì—…ë“¤
        search_tasks = []

        # 1. Vector DB ê²€ìƒ‰
        if self.vector_db:
            search_tasks.append(self._async_vector_search(query))



        # 2. Graph DB ê²€ìƒ‰
        if self.graph_db:
            search_tasks.append(self._async_graph_search(query))

        # 3. ê°„ë‹¨í•œ ì›¹ ê²€ìƒ‰
        search_tasks.append(self._async_web_search(query))



        try:
            # ëª¨ë“  ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            start_time = time.time()
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
            execution_time = time.time() - start_time

            print(f"- ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {execution_time:.2f}ì´ˆ")

            # ê²°ê³¼ ì²˜ë¦¬
            total_results = 0
            for result_group in search_results:
                if isinstance(result_group, Exception):
                    print(f"- ê²€ìƒ‰ ì˜¤ë¥˜: {result_group}")
                    continue

                if isinstance(result_group, list):
                    for result in result_group:
                        state.add_multi_source_result(result)
                        total_results += 1

            # ê°„ë‹¨í•œ LLM ë¶„ì„ (ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ)
            if total_results > 0:
                analysis_result = await self._simple_llm_analysis(
                    query, state.multi_source_results_stream
                )
                if analysis_result:
                    state.add_multi_source_result(analysis_result)
                    total_results += 1

            state.add_step_result("basic_parallel_search", {
                "execution_time": execution_time,
                "total_results": total_results,
                "search_types": len(search_tasks)
            })

            print(f"- ì´ {total_results}ê°œ ê²°ê³¼ ì¶”ê°€")

        except Exception as e:
            print(f"- ê¸°ë³¸ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            fallback_result = self._create_fallback_result(query, "basic_parallel_error")
            state.add_multi_source_result(fallback_result)

        return state

    async def _execute_full_parallel_search(
        self, state: StreamingAgentState, query: str
    ) -> StreamingAgentState:
        """
        ReAct ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰ (COMPLEX ë³µì¡ë„)
        - ë³µì¡í•œ ì§ˆë¬¸ì€ ReAct ì—ì´ì „íŠ¸ì—ê²Œ ëª¨ë“  ê²€ìƒ‰ ë° ì¶”ë¡  ê³¼ì •ì„ ìœ„ì„
        """

        print("\n>> ReAct ì—ì´ì „íŠ¸ ë‹¨ë… ì‹¤í–‰")

        react_task = self._async_react_search(query)

        try:
            start_time = time.time()

            react_result = await react_task

            execution_time = time.time() - start_time
            print(f"- ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {execution_time:.2f}ì´ˆ")

            total_results = 0
            # ReAct ê²°ê³¼ ì²˜ë¦¬
            if not isinstance(react_result, Exception) and react_result:
                state.add_multi_source_result(react_result)
                total_results += 1

            state.add_step_result("full_react_search", {
                "execution_time": execution_time,
                "total_results": total_results,
                "react_included": True
            })

            print(f"- ì´ {total_results}ê°œ ê²°ê³¼ ì¶”ê°€ (ReAct)")

        except Exception as e:
            print(f"- ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            fallback_result = self._create_fallback_result(query, "react_agent_error")
            state.add_multi_source_result(fallback_result)

        return state

    async def _execute_multi_stage_parallel_search(
        self, state: StreamingAgentState, query: str
    ) -> StreamingAgentState:
        """ë‹¤ë‹¨ê³„ ë³‘ë ¬ ê²€ìƒ‰ (SUPER_COMPLEX ë³µì¡ë„)"""
        print("\n>> ë‹¤ë‹¨ê³„ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰")

        try:
            # 1ë‹¨ê³„: ì´ˆê¸° ì •ë³´ ìˆ˜ì§‘ (ë³‘ë ¬)
            print("- 1ë‹¨ê³„: ì´ˆê¸° ì •ë³´ ìˆ˜ì§‘")
            await self._execute_full_parallel_search(state, query)

            # 2ë‹¨ê³„: í‚¤ì›Œë“œ í™•ì¥ ë° ì‹¬í™” ê²€ìƒ‰ (ë³‘ë ¬)
            print("- 2ë‹¨ê³„: í‚¤ì›Œë“œ í™•ì¥ ê²€ìƒ‰")
            expanded_keywords = await self._generate_expanded_keywords(query)

            expanded_search_tasks = []
            for keyword in expanded_keywords[:3]:  # ìƒìœ„ 3ê°œë§Œ
                expanded_search_tasks.extend([
                    self._async_vector_search(keyword),
                    self._async_graph_search(keyword)
                ])

            if expanded_search_tasks:
                expanded_results = await asyncio.gather(
                    *expanded_search_tasks, return_exceptions=True
                )

                for result_group in expanded_results:
                    if isinstance(result_group, list):
                        for result in result_group:
                            state.add_multi_source_result(result)

            # 3ë‹¨ê³„: ì „ëµì  ì¢…í•© ë¶„ì„
            print("- 3ë‹¨ê³„: ì „ëµì  ì¢…í•©")
            synthesis_result = await self._strategic_synthesis(
                query, state.multi_source_results_stream
            )
            if synthesis_result:
                state.add_multi_source_result(synthesis_result)

            # 4ë‹¨ê³„: ìµœì¢… ê²€ì¦
            print("- 4ë‹¨ê³„: ìµœì¢… ê²€ì¦")
            validation_result = await self._final_validation(
                query, state.multi_source_results_stream
            )
            if validation_result:
                state.add_multi_source_result(validation_result)

            state.add_step_result("multi_stage_parallel_search", {
                "stages_completed": 4,
                "expanded_keywords": len(expanded_keywords),
                "total_results": len(state.multi_source_results_stream)
            })

        except Exception as e:
            print(f"- ë‹¤ë‹¨ê³„ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            fallback_result = self._create_fallback_result(query, "multi_stage_error")
            state.add_multi_source_result(fallback_result)

        return state

    # ========== ê°œë³„ ê²€ìƒ‰ ë©”ì„œë“œë“¤ (ë¹„ë™ê¸°) ==========

    async def _async_vector_search(self, query: str) -> List[SearchResult]:
        """ë¹„ë™ê¸° Vector DB ê²€ìƒ‰"""
        try:
            print(f"  â”” Vector DB ê²€ìƒ‰: {query[:30]}...")

            # ìŠ¤ë ˆë“œ í’€ ì‚¬ìš©
            loop = asyncio.get_event_loop()
            vector_results = await loop.run_in_executor(
                self.thread_pool,
                lambda: mock_vector_search.invoke({"query": query})
            )

            results = []
            if isinstance(vector_results, dict) and "results" in vector_results:
                for i, doc in enumerate(vector_results["results"][:3]):
                    result = SearchResult(
                        source="vector_db",
                        content=doc.get("content", ""),
                        relevance_score=doc.get("similarity_score", 0.7),
                        metadata={"search_type": "vector", "rank": i + 1},
                        search_query=query,
                    )
                    results.append(result)

            print(f"    âœ“ Vector DB: {len(results)}ê°œ ê²°ê³¼")
            return results

        except Exception as e:
            print(f"    âœ— Vector DB ì˜¤ë¥˜: {e}")
            return []

    async def _async_graph_search(self, query: str) -> List[SearchResult]:
        """ë¹„ë™ê¸° Graph DB ê²€ìƒ‰"""
        try:
            print(f"  â”” Graph DB ê²€ìƒ‰: {query[:30]}...")

            # í‚¤ì›Œë“œ ìµœì í™”
            keywords = await self._optimize_keywords(query)

            loop = asyncio.get_event_loop()

            results = []
            for keyword in keywords[:2]:  # ìƒìœ„ 2ê°œë§Œ
                graph_result = await loop.run_in_executor(
                    self.thread_pool,
                    lambda k=keyword: graph_db_search.invoke({"query": k})
                )

                if isinstance(graph_result, dict) and "nodes" in graph_result:
                    for node in graph_result["nodes"][:2]:  # ê° í‚¤ì›Œë“œë‹¹ 2ê°œì”©
                        result = SearchResult(
                            source="graph_db",
                            content=f"{node['properties'].get('name', 'Unknown')}: {str(node['properties'])}",
                            relevance_score=0.8,
                            metadata=node,
                            search_query=keyword,
                        )
                        results.append(result)

            print(f"    âœ“ Graph DB: {len(results)}ê°œ ê²°ê³¼")
            return results

        except Exception as e:
            print(f"    âœ— Graph DB ì˜¤ë¥˜: {e}")
            return []

    async def _async_rdb_search(self, query: str) -> List[SearchResult]:
        """ë¹„ë™ê¸° RDB ê²€ìƒ‰"""
        try:
            print(f"  â”” RDB ê²€ìƒ‰: {query[:30]}...")

            # ğŸ”§ 1. RDBìš© ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            processed_query = self._preprocess_rdb_query(query)
            print(f"    â†’ ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬: {processed_query}")

            loop = asyncio.get_event_loop()
            rdb_results_content = await loop.run_in_executor(
                self.thread_pool,
                lambda: rdb_search.invoke({"query": processed_query})
            )

            # ë°˜í™˜ê°’ì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ìœ¼ë¡œ ë³€ê²½
            if isinstance(rdb_results_content, str) and rdb_results_content:
                # ì „ì²´ ë¬¸ìì—´ì„ contentë¡œ í•˜ëŠ” ë‹¨ì¼ SearchResult ê°ì²´ ìƒì„±
                result = SearchResult(
                    source="rdb",
                    content=rdb_results_content,
                    relevance_score=0.85, # DBì—ì„œ ì§ì ‘ ì˜¨ ì •ë³´ì´ë¯€ë¡œ ì‹ ë¢°ë„ ë†’ê²Œ ì„¤ì •
                    metadata={"search_type": "rdb"},
                    search_query=processed_query,
                )
                print(f"    âœ“ RDB: 1ê°œ ê²°ê³¼ ê°ì²´ ìƒì„± ì™„ë£Œ")
                return [result] # ìƒì„±ëœ ê°ì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ë°˜í™˜

            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì˜ˆì™¸ì ì¸ ê²½ìš°ë„ ì²˜ë¦¬
            elif isinstance(rdb_results_content, dict) and "results" in rdb_results_content:
                # ì´ ë¡œì§ì€ ê±°ì˜ ì‹¤í–‰ë˜ì§€ ì•Šê² ì§€ë§Œ, í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
                results = []
                for i, doc in enumerate(rdb_results_content["results"][:2]):
                    results.append(SearchResult(
                        source="rdb",
                        content=doc.get("content", ""),
                        relevance_score=0.8,
                        metadata={"search_type": "rdb", "rank": i + 1},
                        search_query=processed_query,
                    ))
                print(f"    âœ“ RDB (Dict): {len(results)}ê°œ ê²°ê³¼")
                return results

            else:
                print(f"    âœ— RDB: ìœ íš¨í•œ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í•¨ (Type: {type(rdb_results_content)})")
                return []

        except Exception as e:
            print(f"    âœ— RDB ì˜¤ë¥˜: {e}")
            return []

    def _preprocess_rdb_query(self, query: str) -> str:
        """RDB ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ì „ì²˜ë¦¬"""

        # ì ì ˆí•œ ê¸¸ì´ë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if len(query) <= 50:
            return query

        # ê³„íšì„œë‚˜ ë¶„ì„ ë¬¸ì„œ ê°ì§€
        plan_indicators = [
            'ì‹¤í–‰ ê³„íš', 'ë¶„ì„ ì ‘ê·¼ë²•', 'ê²°ê³¼ë¬¼ êµ¬ì„±', 'ì „ëµ', 'ë§ˆì¼€íŒ…',
            'ë³´ê³ ì„œ', 'ì„¹ì…˜', 'ì‹œê°ì  ìë£Œ', 'SWOT', 'ì ‘ê·¼ë²•', 'MZì„¸ëŒ€'
        ]

        if any(indicator in query for indicator in plan_indicators):
            print(f"      â†’ ê³„íšì„œ ë¬¸ì„œ ê°ì§€, í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")

            # ë†ì‚°ë¬¼ í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
            import re
            food_keywords = re.findall(
                r'(ê°ì|ì‚¬ê³¼|ë°°|ì–‘íŒŒ|ë‹¹ê·¼|ë°°ì¶”|ë¬´|ê³ êµ¬ë§ˆ|ì˜¥ìˆ˜ìˆ˜|ìŒ€|ë³´ë¦¬|ë°€|ì½©|íŒ¥|ë”¸ê¸°|í¬ë„|ë³µìˆ­ì•„|ìë‘|ì²´ë¦¬|ìˆ˜ë°•|ì°¸ì™¸|í˜¸ë°•|ì˜¤ì´|í† ë§ˆí† |ìƒì¶”|ì‹œê¸ˆì¹˜|ê¹»ì|ë§ˆëŠ˜|ìƒê°•|íŒŒ|ëŒ€íŒŒ|ìª½íŒŒ|ë¶€ì¶”|ê³ ì¶”|í”¼ë§|íŒŒí”„ë¦¬ì¹´|ê°ê·¤|ê·¤|ì˜¤ë Œì§€|ë°”ë‚˜ë‚˜|í‚¤ìœ„|ë§ê³ )',
                query
            )

            # ê²€ìƒ‰ ì˜ë„ í‚¤ì›Œë“œ ì¶”ì¶œ
            intent_keywords = re.findall(
                r'(ê°€ê²©|ì‹œì„¸|ì˜ì–‘|ì¹¼ë¡œë¦¬|ë¹„íƒ€ë¯¼|ë‹¨ë°±ì§ˆ|ìƒì‚°ëŸ‰|ìˆ˜ê¸‰|ì†Œë¹„|íŠ¸ë Œë“œ|ì‹œì¥|ë¶„ì„)',
                query
            )

            # ì§€ì—­ í‚¤ì›Œë“œ ì¶”ì¶œ
            region_keywords = re.findall(
                r'(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)',
                query
            )

            # ì‹œê°„ í‚¤ì›Œë“œ ì¶”ì¶œ
            time_keywords = re.findall(
                r'(ìµœê·¼|ì˜¤ëŠ˜|ì–´ì œ|ì´ë²ˆì£¼|ì§€ë‚œì£¼|ì´ë²ˆë‹¬|ì§€ë‚œë‹¬|ì˜¬í•´|ì‘ë…„|í˜„ì¬|2024|2025)',
                query
            )

            # í‚¤ì›Œë“œ ì¡°í•©í•˜ì—¬ ê°„ë‹¨í•œ ì¿¼ë¦¬ ìƒì„±
            if food_keywords:
                result_query = food_keywords[0]

                if intent_keywords:
                    result_query += f" {intent_keywords[0]}"

                if time_keywords:
                    result_query = f"{time_keywords[0]} {result_query}"

                if region_keywords:
                    result_query += f" {region_keywords[0]}"

                return result_query

            # ë†ì‚°ë¬¼ì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ê²€ìƒ‰ ì˜ë„
            elif intent_keywords:
                return f"ë†ì‚°ë¬¼ {intent_keywords[0]}"

            else:
                return "ë†ì‚°ë¬¼ ì‹œì¥ ì •ë³´"

        # 3. ì¼ë°˜ì ì¸ ê¸´ ì¿¼ë¦¬ëŠ” ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ
        sentences = query.split('.')
        if sentences and len(sentences[0]) < 100:
            return sentences[0].strip()

        # 4. ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 50ìë§Œ
        return query[:50] + "..."


    async def _async_web_search(self, query: str) -> List[SearchResult]:
        """ë¹„ë™ê¸° ì›¹ ê²€ìƒ‰"""
        try:
            print(f"  â”” Web ê²€ìƒ‰: {query[:30]}...")

            loop = asyncio.get_event_loop()
            web_results = await loop.run_in_executor(
                self.thread_pool,
                lambda: debug_web_search.invoke({"query": query})
            )

            results = []
            if isinstance(web_results, str) and len(web_results) > 50:
                result = SearchResult(
                    source="web_search",
                    content=web_results,
                    relevance_score=0.75,
                    metadata={"search_type": "web"},
                    search_query=query,
                )
                results.append(result)

            print(f"    âœ“ Web: {len(results)}ê°œ ê²°ê³¼")
            return results

        except Exception as e:
            print(f"    âœ— Web ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _async_react_search(self, query: str) -> Optional[SearchResult]:
        """ë¹„ë™ê¸° ReAct ê²€ìƒ‰"""
        try:
            print(f"  â”” ReAct ê²€ìƒ‰: {query[:30]}...")

            if not self.react_agent_executor:
                return None

            enhanced_prompt = self._create_enhanced_query_prompt(query)

            result = await asyncio.wait_for(
                self.react_agent_executor.ainvoke({"input": enhanced_prompt}),
                timeout=120
            )

            output = result.get("output", "")
            if len(output) > 50:
                search_result = SearchResult(
                    source="react_agent",
                    content=output,
                    relevance_score=0.9,
                    metadata={"search_type": "react"},
                    search_query=query,
                )
                print(f"    âœ“ ReAct: ë¶„ì„ ì™„ë£Œ")
                return search_result

            return None

        except Exception as e:
            print(f"    âœ— ReAct ì˜¤ë¥˜: {e}")
            return None



    def _determine_execution_strategy(self, state: StreamingAgentState, complexity_level: str) -> ExecutionStrategy:
        """ì‹¤í–‰ ì „ëµ ê²°ì •"""
        execution_strategy = state.execution_mode
        if not execution_strategy and state.query_plan:
            execution_strategy = state.query_plan.execution_strategy

        if not execution_strategy:
            if complexity_level == "super_complex":
                execution_strategy = ExecutionStrategy.MULTI_AGENT
            elif complexity_level == "complex":
                execution_strategy = ExecutionStrategy.FULL_REACT
            elif complexity_level == "medium":
                execution_strategy = ExecutionStrategy.BASIC_SEARCH
            else:
                execution_strategy = ExecutionStrategy.DIRECT_ANSWER

        return execution_strategy

    async def _optimize_keywords(self, query: str) -> List[str]:
        """Graph DBìš© í‚¤ì›Œë“œ ìµœì í™”"""
        prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— íš¨ê³¼ì ì¸ í•µì‹¬ í‚¤ì›Œë“œ 2-3ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

        ì§ˆë¬¸: {query}

        í‚¤ì›Œë“œë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:
        """

        response = await self.chat.ainvoke(prompt)
        keywords = [kw.strip() for kw in response.content.split(",")]
        return keywords[:3]

    async def _generate_expanded_keywords(self, query: str) -> List[str]:
        """í™•ì¥ í‚¤ì›Œë“œ ìƒì„± (SUPER_COMPLEXìš©)"""
        prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í™•ì¥ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”:

        ì›ë³¸ ì§ˆë¬¸: {query}

        ê´€ë ¨ í‚¤ì›Œë“œ, ìœ ì‚¬ì–´, ìƒìœ„/í•˜ìœ„ ê°œë…ì„ í¬í•¨í•˜ì—¬ 5ê°œì˜ í™•ì¥ í‚¤ì›Œë“œë¥¼ ì œì‹œí•˜ì„¸ìš”:
        """

        response = await self.llm.ainvoke(prompt)
        keywords = [kw.strip() for kw in response.content.split(",")]
        return keywords[:5]

    async def _simple_llm_analysis(self, query: str, search_results: List[SearchResult]) -> Optional[SearchResult]:
        """ê°„ë‹¨í•œ LLM ë¶„ì„"""
        try:
            if not search_results:
                return None

            context = ""
            for result in search_results[-5:]:
                context += f"- {result.source}: {result.content[:200]}\n"

            prompt = f"""
ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ê²€ìƒ‰ ê²°ê³¼:
{context}

ê°„ë‹¨í•œ ë¶„ì„ (200ì ì´ë‚´):
"""

            response = await self.llm.ainvoke(prompt)

            return SearchResult(
                source="llm_analysis",
                content=response.content,
                relevance_score=0.85,
                metadata={"analysis_type": "simple_llm"},
                search_query=query,
            )

        except Exception as e:
            print(f"- LLM ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None

    async def _strategic_synthesis(
        self, query: str, search_results: List[SearchResult]
    ) -> SearchResult:
        """ì „ëµì  ì¢…í•© ë¶„ì„ (SUPER_COMPLEXìš©)"""
        try:
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ì¢…í•©
            context = ""
            for result in search_results:
                context += f"ì¶œì²˜({result.source}): {result.content[:300]}\n\n"

            prompt = f"""
ë‹¹ì‹ ì€ ì „ëµ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì „ëµì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: {query}

ìˆ˜ì§‘ëœ ì •ë³´:
{context}

ì „ëµì  ì¢…í•© ë¶„ì„:
1. í•µì‹¬ ë°œê²¬ì‚¬í•­ (3ê°€ì§€)
2. ì „ëµì  ì‹œì‚¬ì  (2ê°€ì§€)
3. ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ (2ê°€ì§€)

500ì ì´ë‚´ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""

            response = await self.llm.ainvoke(prompt)

            return SearchResult(
                source="strategic_synthesis",
                content=response.content,
                relevance_score=0.95,
                metadata={
                    "analysis_type": "strategic_synthesis",
                    "total_sources": len(search_results),
                    "synthesis_date": self.current_date_str,
                },
                search_query=query,
            )

        except Exception as e:
            print(f"- ì „ëµì  ì¢…í•© ì˜¤ë¥˜: {e}")
            return None

    async def _final_validation(
        self, query: str, search_results: List[SearchResult]
    ) -> SearchResult:
        """ìµœì¢… ê²€ì¦ ë¶„ì„ (SUPER_COMPLEXìš©)"""
        try:
            # ìµœê·¼ ë¶„ì„ ê²°ê³¼ë“¤ë§Œ ê²€ì¦
            recent_results = (
                search_results[-3:] if len(search_results) >= 3 else search_results
            )

            context = ""
            for result in recent_results:
                context += f"{result.source}: {result.content[:200]}\n"

            prompt = f"""
ìµœì¢… ê²€ì¦ìë¡œì„œ ë¶„ì„ ê²°ê³¼ì˜ ì¼ê´€ì„±ê³¼ ì™„ì„±ë„ë¥¼ í‰ê°€í•˜ê³  ë³´ì™„í•˜ì„¸ìš”.

ì›ë³¸ ìš”ì²­: {query}

ë¶„ì„ ê²°ê³¼ë“¤:
{context}

ìµœì¢… ê²€ì¦ ë° ë³´ì™„:
- ë¶„ì„ì˜ ì¼ê´€ì„± ê²€í† 
- ëˆ„ë½ëœ ì¤‘ìš” ê´€ì  ì‹ë³„
- ìµœì¢… ê²°ë¡  ë° ê¶Œê³ ì‚¬í•­

300ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”.
"""

            response = await self.llm.ainvoke(prompt)

            return SearchResult(
                source="final_validation",
                content=response.content,
                relevance_score=0.9,
                metadata={
                    "analysis_type": "final_validation",
                    "validated_sources": len(recent_results),
                    "validation_date": self.current_date_str,
                },
                search_query=query,
            )

        except Exception as e:
            print(f"- ìµœì¢… ê²€ì¦ ì˜¤ë¥˜: {e}")
            return None

    def _create_enhanced_query_prompt(self, query: str) -> str:
        """ReActìš© í–¥ìƒëœ í”„ë¡¬í”„íŠ¸"""
        return f"""
    Research this topic thoroughly: {query}

    Current Date: {self.current_date_str}

    Please use the available tools to research this topic.

    IMPORTANT: Use this exact format for tools:
    Action: tool_name
    Action Input: search_query

    DO NOT use parentheses or function call syntax.

    Begin your research now.
    """

    def _create_react_agent(self):
        """ReAct ì—ì´ì „íŠ¸ ìƒì„±"""
        try:
            # ê¸°ë³¸ ReAct í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            base_prompt = hub.pull("hwchase17/react")

            # ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­
            system_instruction = f"""
    You are an expert research assistant for agricultural and food industry analysis.
    Current Date: {self.current_date_str}

    CRITICAL: TOOL USAGE FORMAT
    When using tools, you MUST follow this EXACT format:

    Action: tool_name
    Action Input: your_query_here

    NEVER use function call syntax like tool_name("query") - this will cause errors.

    CORRECT EXAMPLES:
    Action: debug_web_search
    Action Input: MZì„¸ëŒ€ ì†Œë¹„ íŒ¨í„´ 2025

    Action: mock_vector_search
    Action Input: ë†ì‚°ë¬¼ ê°€ê²© ë™í–¥ ë¶„ì„

    Action: rdb_search
    Action Input: ì‚¬ê³¼ ì˜ì–‘ì„±ë¶„ ë°ì´í„°

    Action: graph_db_search
    Action Input: ë†ì—… ì—°êµ¬ê¸°ê´€ ê´€ê³„ ë¶„ì„

    AVAILABLE TOOLS:
    1. debug_web_search - For latest web information, current events, breaking news
    2. mock_vector_search - For document content analysis, research papers, news articles
    3. rdb_search - For structured data, statistics, numerical information
    4. graph_db_search - For entity relationships, knowledge graph analysis

    RESEARCH STRATEGY:
    1. Start with the most relevant tool for the query type
    2. Use multiple tools if comprehensive analysis is needed
    3. Always analyze results before providing final answer
    4. Synthesize information from all sources

    Remember: Use the EXACT Action/Action Input format shown above.
    """

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
            react_prompt = PromptTemplate(
                template=system_instruction + "\n\n" + base_prompt.template,
                input_variables=base_prompt.input_variables
            )

            # ReAct ì—ì´ì „íŠ¸ ìƒì„±
            react_agent_runnable = create_react_agent(
                self.llm, self.available_tools, react_prompt
            )

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° - ë” ê´€ëŒ€í•œ ì„¤ì •
            return AgentExecutor(
                agent=react_agent_runnable,
                tools=self.available_tools,
                verbose=True,
                handle_parsing_errors=True,  # íŒŒì‹± ì—ëŸ¬ ìë™ ì²˜ë¦¬
                max_iterations=6,  # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€
                max_execution_time=200,  # ì‹¤í–‰ ì‹œê°„ ì¶©ë¶„íˆ í™•ë³´
                early_stopping_method="generate",
                return_intermediate_steps=True,
            )

        except Exception as e:
            print(f"ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None


    def _create_fallback_result(self, query: str, error_type: str) -> SearchResult:
        """í´ë°± ê²°ê³¼ ìƒì„±"""
        return SearchResult(
            source=f"fallback_{error_type}",
            content=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸: {query}",
            relevance_score=0.3,
            metadata={"error_type": error_type},
            search_query=query,
        )


# CriticAgent1: ì •ë³´ëŸ‰ ì¶©ë¶„ì„± í‰ê°€
class CriticAgent1:
    def __init__(self):
        self.chat = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.agent_type = AgentType.CRITIC_1

    async def evaluate(self, state: StreamingAgentState) -> StreamingAgentState:
        print(">> CRITIC_1 ì‹œì‘")
        graph_results = state.graph_results_stream
        multi_results = state.multi_source_results_stream
        print(f"- Graph DB ê²°ê³¼: {len(graph_results)}ê°œ")
        print(f"- Multi Source ê²°ê³¼: {len(multi_results)}ê°œ")

        evaluation_result = await self._evaluate_sufficiency(
            state.original_query, graph_results, multi_results
        )

        print(f"- í‰ê°€ ê²°ê³¼: {evaluation_result.get('status', 'insufficient')}")
        print(f"- í‰ê°€ ì‹ ë¢°ë„: {evaluation_result.get('confidence', 0.0)}")
        print(f"- í‰ê°€ ì´ìœ : {evaluation_result.get('reasoning', 'N/A')}")

        state.critic1_result = CriticResult(**evaluation_result)


        if evaluation_result.get("status") == "sufficient":
            state.info_sufficient = True
            print(
                "- ì •ë³´ê°€ ì¶©ë¶„í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
            )
        else:
            state.info_sufficient = False
            print("- ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì¶”ê°€ ê²€ìƒ‰ì„ ìš”ì²­í•©ë‹ˆë‹¤.")

            if evaluation_result.get("status") == "insufficient":
                print(f"- ê°œì„  ì œì•ˆ: {evaluation_result.get('suggestion', 'N/A')}")

        memory = state.get_agent_memory(AgentType.CRITIC_1)
        memory.add_finding(f"ì •ë³´ ì¶©ë¶„ì„± í‰ê°€: {state.info_sufficient}")
        memory.update_metric(
            "confidence_score", evaluation_result.get("confidence", 0.5)
        )
        print(">> CRITIC_1 ì™„ë£Œ")
        return state

    def _summarize_results(self, results, source_name):
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ í—¬í¼ í•¨ìˆ˜"""
        if not results:
            return f"{source_name}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\n"

        summary = f"{source_name} ({len(results)}ê°œ ê²°ê³¼):\n"
        for r in results[:3]:
            content_preview = r.content[:100].strip().replace("\n", " ")
            summary += f"  - {content_preview}...\n"
        return summary

    async def _evaluate_sufficiency(self, original_query, graph_results, multi_results):
        results_summary = self._summarize_results(
            graph_results, "Graph DB"
        ) + self._summarize_results(multi_results, "Multi-Source (ReAct Agent)")

        prompt = f"""
        ë‹¹ì‹ ì€ ìˆ˜ì§‘ëœ ì •ë³´ê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— "ëŒ€ì²´ë¡œ ì¶©ë¶„í•œ ìˆ˜ì¤€ì¸ì§€"ë¥¼ ì‹¤ìš©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìˆ˜ì„ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì™„ë²½í•˜ì§€ ì•Šë”ë¼ë„, í•µì‹¬ì ì¸ ë‹µë³€ ìƒì„±ì´ ê°€ëŠ¥í•œì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

        ### [ë§¤ìš° ì¤‘ìš”í•œ íŒë‹¨ ê¸°ì¤€]
        - ì •ë³´ê°€ ì•½ 80% ì´ìƒ í¬í•¨ë˜ì–´ ìˆê³ , ì§ˆë¬¸ì˜ í•µì‹¬ ë…¼ì§€ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤ë©´ **'sufficient'ë¡œ íŒë‹¨**í•˜ì„¸ìš”.
        - ì¼ë¶€ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆë”ë¼ë„, ë‹µë³€ì˜ ì „ì²´ì ì¸ íë¦„ì„ ë§Œë“œëŠ” ë° ì§€ì¥ì´ ì—†ë‹¤ë©´ **'sufficient'ë¡œ íŒë‹¨**í•˜ì„¸ìš”.
        - ì •ë³´ê°€ ì „í˜€ ì—†ê±°ë‚˜, ì§ˆë¬¸ì˜ ì£¼ì œì™€ ì™„ì „íˆ ë™ë–¨ì–´ì§„ ë‚´ìš©ì¼ ê²½ìš°ì—ë§Œ **'insufficient'ë¡œ íŒë‹¨**í•˜ì„¸ìš”.
        ---

        <ì›ë³¸ ì§ˆë¬¸>
        "{original_query}"

        <ìˆ˜ì§‘ëœ ì •ë³´ ìš”ì•½>
        {results_summary}
        ---

        **[í‰ê°€ ê²°ê³¼]** (ì•„ë˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.)
        STATUS: sufficient ë˜ëŠ” insufficient
        REASONING: [íŒë‹¨ ê·¼ê±°ë¥¼ ìœ„ ê¸°ì¤€ì— ë§ì¶° ê°„ê²°í•˜ê²Œ ì‘ì„±]
        SUGGESTION: [STATUSê°€ 'insufficient'ì¼ ê²½ìš°ì—ë§Œ, ë‹¤ìŒ ê²€ìƒ‰ì— ë„ì›€ì´ ë  êµ¬ì²´ì ì¸ ì œì•ˆì„ ì‘ì„±. 'sufficient'ì¼ ê²½ìš° 'ì—†ìŒ'ìœ¼ë¡œ ì‘ì„±.]
        CONFIDENCE: [ë‹¹ì‹ ì˜ 'STATUS' íŒë‹¨ì— ëŒ€í•œ ì‹ ë¢°ë„ë¥¼ 0.0 ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‘œí˜„. ì ìˆ˜ê°€ 0.85 ì´ìƒì´ë©´ ë§¤ìš° í™•ì‹ í•˜ëŠ” ìƒíƒœì„.]
        """
        response = await self.chat.ainvoke(prompt)
        return self._parse_evaluation(response.content)

    def _parse_evaluation(self, response_content):
        try:
            lines = response_content.strip().split("\n")
            result = {}
            for line in lines:
                if line.startswith("STATUS:"):
                    result["status"] = line.split(":", 1)[1].strip()
                elif line.startswith("REASONING:"):
                    result["reasoning"] = line.split(":", 1)[1].strip()
                elif line.startswith("SUGGESTION:"):
                    result["suggestion"] = line.split(":", 1)[1].strip()
                elif line.startswith("CONFIDENCE:"):
                    try:
                        result["confidence"] = float(line.split(":", 1)[1].strip())
                    except:
                        result["confidence"] = 0.5

            if "status" not in result:
                result["status"] = "insufficient"
            if "reasoning" not in result:
                result["reasoning"] = "íŒë‹¨ ê·¼ê±° ì—†ìŒ"
            if result.get("status") == "insufficient" and not result.get("suggestion"):
                result["suggestion"] = "ë‚´ìš© ë³´ê°•ì´ í•„ìš”í•©ë‹ˆë‹¤."

            return result
        except Exception as e:
            print(f"- íŒŒì‹± ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return {
                "status": "insufficient",
                "reasoning": "í‰ê°€ íŒŒì‹± ì‹¤íŒ¨",
                "suggestion": "ë§¥ë½ ì¬êµ¬ì„± ê¶Œì¥",
                "confidence": 0.5,
            }


# ContextIntegratorAgent
class ContextIntegratorAgent:
    def __init__(self):
        # ìµœì¢… ë³´ê³ ì„œ ì´ˆì•ˆ ì‘ì„±ì´ë¯€ë¡œ ë” ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŒ
        self.chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
        self.agent_type = AgentType.CONTEXT_INTEGRATOR

    async def integrate(self, state: StreamingAgentState) -> StreamingAgentState:
        """ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì˜ 'ì´ˆì•ˆ'ì„ ìƒì„±"""
        print(">> CONTEXT_INTEGRATOR ì‹œì‘ (ë‹µë³€ ì´ˆì•ˆ ìƒì„±)")

        graph_results = state.graph_results_stream
        multi_results = state.multi_source_results_stream
        all_results = graph_results + multi_results

        if not all_results:
            print("- í†µí•©í•  ê²°ê³¼ê°€ ì—†ìŒ")
            state.integrated_context = (
                "ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ì–´ ë‹µë³€ ì´ˆì•ˆì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            return state

        print(f"- ì´ {len(all_results)}ê°œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ˆì•ˆ ì‘ì„± ì‹œì‘")

        # PostgreSQL ê²°ê³¼ë¥¼ ë¨¼ì € íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
        structured_data = self._parse_postgresql_results(all_results)
        print(f"- PostgreSQL êµ¬ì¡°í™” ë°ì´í„°: {len(structured_data.get('nutrition_data', []))}ê±´ ì˜ì–‘ì†Œ, {len(structured_data.get('price_data', []))}ê±´ ê°€ê²©")

        # _create_draft í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì´ˆì•ˆì„ ìƒì„±
        draft = await self._create_draft(state.original_query, all_results, structured_data)

        # ìƒì„±ëœ ì´ˆì•ˆì„ integrated_contextì— ì €ì¥
        state.integrated_context = draft

        print(f"- ë‹µë³€ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(draft)}ì)")
        print("\n>> CONTEXT_INTEGRATOR ì™„ë£Œ")
        return state

    def _parse_postgresql_results(self, all_results: list) -> dict:
        """PostgreSQL ê²€ìƒ‰ ê²°ê³¼ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ"""
        structured_data = {
            'nutrition_data': [],
            'price_data': [],
            'other_data': []
        }

        for result in all_results:
            content = result.content

            try:
                # PostgreSQL ê²°ê³¼ì¸ì§€ í™•ì¸
                if 'PostgreSQL ê²€ìƒ‰ ê²°ê³¼' in content:
                    # ì •í™•í•œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
                    json_match = re.search(r'### ìƒì„¸ ë°ì´í„° \(JSON\)\s*(\{.*?\n\})', content, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                        data = json.loads(json_content)

                        # ì˜ì–‘ì†Œ ë°ì´í„° ì¶”ì¶œ
                        if 'nutrition_data' in data and data['nutrition_data']:
                            for item in data['nutrition_data']:
                                structured_item = {
                                    'ì‹í’ˆëª…': item.get('ì‹í’ˆëª…', 'N/A'),
                                    'ì‹í’ˆêµ°': item.get('ì‹í’ˆêµ°', 'N/A'),
                                    'ì¶œì²˜': item.get('ì¶œì²˜', 'N/A'),
                                    'ì¹¼ë¡œë¦¬': item.get('ì¹¼ë¡œë¦¬', 0),
                                    'ë‹¨ë°±ì§ˆ': item.get('ë‹¨ë°±ì§ˆ', 0),
                                    'ì§€ë°©': item.get('ì§€ë°©', 0),
                                    'íƒ„ìˆ˜í™”ë¬¼': item.get('íƒ„ìˆ˜í™”ë¬¼', 0),
                                    'ì‹ì´ì„¬ìœ ': item.get('ì‹ì´ì„¬ìœ ', 0),
                                    'ì¹¼ìŠ˜': item.get('ì¹¼ìŠ˜', 0),
                                    'ì² ': item.get('ì² ', 0),
                                    'ë‚˜íŠ¸ë¥¨': item.get('ë‚˜íŠ¸ë¥¨', 0),
                                    'ì¹¼ë¥¨': item.get('ì¹¼ë¥¨', 0),
                                    'ë§ˆê·¸ë„¤ìŠ˜': item.get('ë§ˆê·¸ë„¤ìŠ˜', 0),
                                    'ë¹„íƒ€ë¯¼b1': item.get('ë¹„íƒ€ë¯¼b1', 0),
                                    'ë¹„íƒ€ë¯¼b2': item.get('ë¹„íƒ€ë¯¼b2', 0),
                                    'ë¹„íƒ€ë¯¼b6': item.get('ë¹„íƒ€ë¯¼b6', 0),
                                    'ë¹„íƒ€ë¯¼c': item.get('ë¹„íƒ€ë¯¼c', 0),
                                    'ë¹„íƒ€ë¯¼e': item.get('ë¹„íƒ€ë¯¼e', 0),
                                    'ì—½ì‚°': item.get('ì—½ì‚°', 0)
                                }
                                structured_data['nutrition_data'].append(structured_item)

                        # ê°€ê²© ë°ì´í„° ì¶”ì¶œ
                        if 'price_data' in data and data['price_data']:
                            for item in data['price_data']:
                                structured_item = {
                                    'í’ˆëª©ëª…': item.get('product_cls_name', 'N/A'),
                                    'ì¹´í…Œê³ ë¦¬': item.get('category_name', 'N/A'),
                                    'ê°€ê²©': item.get('value', 0),
                                    'ë‹¨ìœ„': item.get('unit', 'kg'),
                                    'ë‚ ì§œ': item.get('regday', 'N/A')
                                }
                                structured_data['price_data'].append(structured_item)

            # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
            except (json.JSONDecodeError, AttributeError) as e:
                print(f"- PostgreSQL ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë‚´ìš©ì„ other_dataì— ì¶”ê°€
                structured_data['other_data'].append({
                    'source': result.source,
                    'content': content[:500] + "..." if len(content) > 500 else content
                })

        return structured_data


    async def _create_draft(self, original_query: str, all_results: list, structured_data: dict) -> str:
        """ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì˜ ì´ˆì•ˆì„ ì‘ì„± - PostgreSQL ë°ì´í„° ìš°ì„  í™œìš©"""

        # 1. PostgreSQL êµ¬ì¡°í™” ë°ì´í„°ë¥¼ ìš°ì„  ì²˜ë¦¬
        postgresql_summary = self._format_postgresql_data(structured_data)

        # 2. ë‹¤ë¥¸ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ (PostgreSQL ì œì™¸)
        other_results_summary = ""
        non_postgresql_count = 0
        for result in all_results[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€
            if hasattr(result, 'source') and result.source == 'rdb':
                continue  # PostgreSQL ê²°ê³¼ëŠ” ì´ë¯¸ êµ¬ì¡°í™”í•´ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ

            source_name = getattr(result, 'source', 'Unknown')
            content = getattr(result, 'content', str(result))
            other_results_summary += f"- ì¶œì²˜({source_name}): {content[:300]}...\n"
            non_postgresql_count += 1

        prompt = f"""
        ë‹¹ì‹ ì€ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ ë³µì¡í•œ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ 'ì´ˆì•ˆ'ì„ ì‘ì„±í•˜ëŠ” ìˆ˜ì„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

        ### ì¤‘ìš”: PostgreSQL ë†ì§„ì²­ ë°ì´í„° ìµœìš°ì„  í™œìš©

        **[ì›ë³¸ ì§ˆë¬¸]**
        {original_query}

        **[1ìˆœìœ„: PostgreSQL êµ¬ì¡°í™” ë°ì´í„° - ë°˜ë“œì‹œ ìš°ì„  í™œìš©]**
        {postgresql_summary}

        **[2ìˆœìœ„: ê¸°íƒ€ ê²€ìƒ‰ ê²°ê³¼ - ë³´ì™„ì  í™œìš©]**
        {other_results_summary if other_results_summary else "ê¸°íƒ€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"}

        ### ì‘ì—… ì§€ì¹¨:
        1. **PostgreSQL ë†ì§„ì²­ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ ìµœìš°ì„ ìœ¼ë¡œ í™œìš©**í•˜ì—¬ ë‹µë³€ ì‘ì„±
        2. ì˜ì–‘ì†Œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì •í™•í•œ ìˆ˜ì¹˜(ì¹¼ë¡œë¦¬, ë‹¨ë°±ì§ˆ ë“±)ë¥¼ ë°˜ë“œì‹œ í¬í•¨
        3. ê°€ê²© ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìµœì‹  ì‹œì„¸ ì •ë³´ë¥¼ í¬í•¨
        4. ë†ì§„ì²­ ì¶œì²˜ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ "(ì¶œì²˜: ë†ì§„ì²­ 'XX, RDB)" í˜•íƒœë¡œ ëª…ì‹œ
        5. ê¸°íƒ€ ê²€ìƒ‰ ê²°ê³¼ëŠ” PostgreSQL ë°ì´í„°ë¥¼ ë³´ì™„í•˜ëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©
        6. ì„œë¡ , ë³¸ë¡ , ê²°ë¡ ì˜ êµ¬ì¡°ë¥¼ ê°–ì¶˜ ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª…ê¸€ í˜•ì‹ ì‘ì„±
        7. ì‹¤ì œ ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ ì ˆëŒ€ ë‹¤ë¥¸ ìˆ˜ì¹˜ë¡œ ëŒ€ì²´í•˜ì§€ ë§ ê²ƒ

        ### ê¸ˆì§€ì‚¬í•­:
        - PostgreSQLì— ì •í™•í•œ ë†ì§„ì²­ ë°ì´í„°ê°€ ìˆëŠ”ë° ë‹¤ë¥¸ ìˆ˜ì¹˜ ì‚¬ìš© ê¸ˆì§€
        - ì¼ë°˜ì ì¸ í•´ì™¸ ë°ì´í„°(USDA ë“±)ë¥¼ ë†ì§„ì²­ ë°ì´í„°ë³´ë‹¤ ìš°ì„  ì‚¬ìš© ê¸ˆì§€
        - ì¶”ì •ì¹˜ë‚˜ ì„ì˜ ìˆ˜ì¹˜ë¥¼ ì‹¤ì œ ë°ì´í„° ëŒ€ì‹  ì‚¬ìš© ê¸ˆì§€

        **[ë‹µë³€ ì´ˆì•ˆ ì‘ì„±]**
        """

        response = await self.chat.ainvoke(prompt)
        return response.content

    def _format_postgresql_data(self, structured_data: dict) -> str:
        """PostgreSQL êµ¬ì¡°í™” ë°ì´í„°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        formatted = ""

        # ì˜ì–‘ì†Œ ë°ì´í„° í¬ë§·íŒ…
        nutrition_data = structured_data.get('nutrition_data', [])
        if nutrition_data:
            formatted += "### ë†ì§„ì²­ ì˜ì–‘ì†Œ ë°ì´í„° (PostgreSQL):\n"
            for item in nutrition_data:
                formatted += f"**{item['ì‹í’ˆëª…']}** ({item['ì‹í’ˆêµ°']})\n"
                formatted += f"- ì¶œì²˜: {item['ì¶œì²˜']}\n"
                formatted += f"- ì¹¼ë¡œë¦¬: {item['ì¹¼ë¡œë¦¬']}kcal/100g\n"
                formatted += f"- ë‹¨ë°±ì§ˆ: {item['ë‹¨ë°±ì§ˆ']}g/100g\n"
                formatted += f"- ì§€ë°©: {item['ì§€ë°©']}g/100g\n"
                formatted += f"- íƒ„ìˆ˜í™”ë¬¼: {item['íƒ„ìˆ˜í™”ë¬¼']}g/100g\n"
                formatted += f"- ì‹ì´ì„¬ìœ : {item['ì‹ì´ì„¬ìœ ']}g/100g\n"

                # ë¯¸ë„¤ë„ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if item['ì¹¼ìŠ˜'] or item['ì² '] or item['ë§ˆê·¸ë„¤ìŠ˜']:
                    formatted += f"- ì¹¼ìŠ˜: {item['ì¹¼ìŠ˜']}mg, ì² : {item['ì² ']}mg, ë§ˆê·¸ë„¤ìŠ˜: {item['ë§ˆê·¸ë„¤ìŠ˜']}mg\n"

                # ë¹„íƒ€ë¯¼ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if item['ë¹„íƒ€ë¯¼b1'] or item['ë¹„íƒ€ë¯¼b2'] or item['ë¹„íƒ€ë¯¼e']:
                    formatted += f"- ë¹„íƒ€ë¯¼B1: {item['ë¹„íƒ€ë¯¼b1']}mg, ë¹„íƒ€ë¯¼B2: {item['ë¹„íƒ€ë¯¼b2']}mg, ë¹„íƒ€ë¯¼E: {item['ë¹„íƒ€ë¯¼e']}mg\n"

                formatted += "\n"

        # ê°€ê²© ë°ì´í„° í¬ë§·íŒ…
        price_data = structured_data.get('price_data', [])
        if price_data:
            formatted += "### ë†ìˆ˜ì‚°ë¬¼ ê°€ê²© ë°ì´í„° (PostgreSQL):\n"
            for item in price_data:
                formatted += f"**{item['í’ˆëª©ëª…']}** ({item['ì¹´í…Œê³ ë¦¬']})\n"
                formatted += f"- ê°€ê²©: {item['ê°€ê²©']}ì›/{item['ë‹¨ìœ„']}\n"
                formatted += f"- ë‚ ì§œ: {item['ë‚ ì§œ']}\n\n"

        # ê¸°íƒ€ ë°ì´í„°
        other_data = structured_data.get('other_data', [])
        if other_data:
            formatted += "### ê¸°íƒ€ RDB ë°ì´í„°:\n"
            for item in other_data:
                formatted += f"- {item['content']}\n"

        if not formatted:
            formatted = "PostgreSQL êµ¬ì¡°í™” ë°ì´í„° ì—†ìŒ\n"

        return formatted



# ë¦¬íŒ©í† ë§ ê´€ë ¨ ì„í¬íŠ¸(ì°¸ê³ ìš©)
from ...core.config.report_config import TeamType, ReportType, Language
from ...services.templates.report_templates import ReportTemplateManager
from ...services.builders.prompt_builder import PromptBuilder
from ...utils.analyzers.query_analyzer import QueryAnalyzer


class ReportGeneratorAgent:
    """ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self):
        self.streaming_chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.9, streaming=True)
        self.non_streaming_chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
        self.agent_type = "REPORT_GENERATOR"

        self.template_manager = ReportTemplateManager()
        self.prompt_builder = PromptBuilder(self.template_manager)
        self.data_extractor = DataExtractor()

    async def generate_streaming_with_sources(
        self,
        state: StreamingAgentState,
        source_collection_data: Dict = None
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë³´ê³ ì„œ ìƒì„± - ë©”ì¸ ë¡œì§"""

        print("\n>> REFACTORED REPORT_GENERATOR ì‹œì‘")

        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        integrated_context = state.integrated_context
        original_query = state.original_query
        memory_context = getattr(state, "memory_context", "")
        user_context = getattr(state, "user_context", None)

        print(f"- ì¿¼ë¦¬: {original_query[:50]}...")
        print(f"- ì»¨í…ìŠ¤íŠ¸: {len(integrated_context)}ì")

        if not integrated_context:
            error_msg = "ë¶„ì„í•  ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            state.final_answer = error_msg
            yield error_msg
            return

        # 1. ì¿¼ë¦¬ ë¶„ì„
        team_type = QueryAnalyzer.detect_team_type(original_query)
        language = QueryAnalyzer.detect_language(original_query)
        complexity_analysis = QueryAnalyzer.analyze_complexity(original_query, user_context)
        report_type = complexity_analysis["report_type"]

        print(f"- ë¶„ì„ ê²°ê³¼: {team_type.value} / {report_type.value} / {language.value}")

        # 2. ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
        all_results = getattr(state, 'graph_results_stream', []) + getattr(state, 'multi_source_results_stream', [])
        extracted_data = await self.data_extractor.extract_numerical_data(all_results, original_query)

        print(f"- ì¶”ì¶œëœ ìˆ˜ì¹˜: {len(extracted_data.get('extracted_numbers', []))}ê°œ")

        # 3. ì°¨íŠ¸ ìƒì„±
        real_charts = await self._create_data_driven_charts(extracted_data, original_query)
        print(f"- ìƒì„±ëœ ì°¨íŠ¸: {len(real_charts)}ê°œ")

        # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.prompt_builder.build_prompt(
            query=original_query,
            context=integrated_context,
            team_type=team_type,
            report_type=report_type,
            language=language,
            extracted_data=extracted_data,
            real_charts=real_charts,
            source_data=source_collection_data
        )

        # 5. ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        full_response = ""
        try:
            print("- ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
            async for chunk in self.streaming_chat.astream(prompt):
                if chunk.content:
                    full_response += chunk.content
                    yield chunk.content
        except Exception as e:
            error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
            yield error_msg
            full_response = error_msg

        state.final_answer = full_response
        print(f"- ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ (ì´ {len(full_response)}ì)")

    async def _create_data_driven_charts(self, extracted_data: Dict, query: str) -> List[Dict]:
        """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì°¨íŠ¸ ìƒì„± - ë°ì´í„° ê²€ì¦ ë¡œì§ ê°•í™”"""
        charts = []

        print(f"\n>> ì°¨íŠ¸ ìƒì„± ì‹œì‘")
        print(f"- ì¶”ì¶œëœ ë°ì´í„°: {extracted_data}")

        if not extracted_data:
            print("- ì¶”ì¶œëœ ë°ì´í„° ì—†ìŒ, ë¹ˆ ì°¨íŠ¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return charts

        # 1. í¼ì„¼íŠ¸ ë°ì´í„° -> íŒŒì´ ì°¨íŠ¸
        percentages = extracted_data.get('percentages', [])
        if len(percentages) >= 2:
            print(f"- í¼ì„¼íŠ¸ ë°ì´í„° ë°œê²¬: {len(percentages)}ê°œ")
            # ë°ì´í„° ê²€ì¦
            valid_percentages = [
                p for p in percentages
                if isinstance(p.get('value'), (int, float)) and 0 <= p.get('value') <= 100
            ]

            if len(valid_percentages) >= 2:
                labels = []
                values = []

                for p in valid_percentages[:5]:
                    context = p.get('context', 'í•­ëª©')
                    if len(context) > 20:
                        context = context[:20] + "..."
                    labels.append(context)
                    values.append(float(p['value']))

                chart = {
                    "title": f"{query[:30]}... ë¹„ìœ¨ ë¶„ì„ (ì‹¤ì œ ë°ì´í„°)",
                    "type": "pie",
                    "data": {
                        "labels": labels,
                        "datasets": [{"label": "ë¹„ìœ¨ (%)", "data": values}]
                    },
                    "source": "ì‹¤ì œ ì¶”ì¶œ ë°ì´í„°",
                    "data_type": "real"
                }
                charts.append(chart)
                print(f"- íŒŒì´ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {chart['title']}")

        # 2. ì¼ë°˜ ìˆ˜ì¹˜ ë°ì´í„° -> ë°” ì°¨íŠ¸
        numbers = extracted_data.get('extracted_numbers', [])
        if len(numbers) >= 2:
            print(f"- ìˆ˜ì¹˜ ë°ì´í„° ë°œê²¬: {len(numbers)}ê°œ")
            # ë°ì´í„° ê²€ì¦
            valid_numbers = [
                n for n in numbers
                if isinstance(n.get('value'), (int, float))
            ]

            if len(valid_numbers) >= 2:
                labels = []
                values = []
                units = []

                for n in valid_numbers[:5]:
                    context = n.get('context', 'í•­ëª©')
                    if len(context) > 15:
                        context = context[:15] + "..."
                    labels.append(context)
                    values.append(float(n['value']))
                    units.append(n.get('unit', ''))

                # ë‹¨ìœ„ í†µì¼ (ì²« ë²ˆì§¸ ë‹¨ìœ„ ì‚¬ìš©)
                primary_unit = units[0] if units[0] else 'ë‹¨ìœ„'

                chart = {
                    "title": f"{query[:30]}... ì£¼ìš” ìˆ˜ì¹˜ (ì‹¤ì œ ë°ì´í„°)",
                    "type": "bar",
                    "data": {
                        "labels": labels,
                        "datasets": [{"label": f"ìˆ˜ì¹˜ ({primary_unit})", "data": values}]
                    },
                    "source": "ì‹¤ì œ ì¶”ì¶œ ë°ì´í„°",
                    "data_type": "real"
                }
                charts.append(chart)
                print(f"- ë°” ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {chart['title']}")

        # 3. íŠ¸ë Œë“œ ë°ì´í„° -> ë¼ì¸ ì°¨íŠ¸
        trends = extracted_data.get('trends', [])
        if len(trends) >= 2:
            print(f"- íŠ¸ë Œë“œ ë°ì´í„° ë°œê²¬: {len(trends)}ê°œ")

            labels = []
            values = []

            for t in trends[:6]:
                period = t.get('period', 'ê¸°ê°„')
                labels.append(period)

                # ë³€í™”ìœ¨ ì¶”ì¶œ
                change_str = str(t.get('change', '0'))
                import re
                numbers = re.findall(r'-?\d+\.?\d*', change_str)
                change_value = float(numbers[0]) if numbers else 0
                values.append(change_value)

            if len(values) >= 2:
                chart = {
                    "title": f"{query[:30]}... ì‹œê°„ë³„ ë³€í™” ì¶”ì´ (ì‹¤ì œ ë°ì´í„°)",
                    "type": "line",
                    "data": {
                        "labels": labels,
                        "datasets": [{"label": "ë³€í™”ìœ¨ (%)", "data": values}]
                    },
                    "source": "ì‹¤ì œ ì¶”ì¶œ ë°ì´í„°",
                    "data_type": "real"
                }
                charts.append(chart)
                print(f"- ë¼ì¸ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {chart['title']}")

        print(f"- ì´ {len(charts)}ê°œ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ")
        return charts

    def _validate_chart_data(self, chart: Dict) -> bool:
        """ì°¨íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['title', 'type', 'data', 'source', 'data_type']
            for field in required_fields:
                if field not in chart:
                    print(f"- ì°¨íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {field} í•„ë“œ ì—†ìŒ")
                    return False

            # ë°ì´í„° êµ¬ì¡° í™•ì¸
            data = chart['data']
            if 'labels' not in data or 'datasets' not in data:
                print(f"- ì°¨íŠ¸ ê²€ì¦ ì‹¤íŒ¨: ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜")
                return False

            # ë°ì´í„° ê¸¸ì´ í™•ì¸
            labels = data['labels']
            datasets = data['datasets']

            if not labels or not datasets:
                print(f"- ì°¨íŠ¸ ê²€ì¦ ì‹¤íŒ¨: ë¹ˆ ë°ì´í„°")
                return False

            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì˜ ë°ì´í„° ê¸¸ì´ì™€ ë¼ë²¨ ê¸¸ì´ ì¼ì¹˜ í™•ì¸
            if len(datasets) > 0 and 'data' in datasets[0]:
                if len(labels) != len(datasets[0]['data']):
                    print(f"- ì°¨íŠ¸ ê²€ì¦ ì‹¤íŒ¨: ë¼ë²¨ê³¼ ë°ì´í„° ê¸¸ì´ ë¶ˆì¼ì¹˜")
                    return False

            # JSON ì§ë ¬í™” ê°€ëŠ¥ í™•ì¸
            json.dumps(chart, ensure_ascii=False)

            print(f"- ì°¨íŠ¸ ê²€ì¦ ì„±ê³µ: {chart['title']}")
            return True

        except Exception as e:
            print(f"- ì°¨íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return False

class SimpleAnswererAgent:
    """ë‹¨ìˆœ ì§ˆë¬¸ ì „ìš© Agent - ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì§€ì›"""

    def __init__(self, vector_db=None):
        self.vector_db = vector_db
        self.streaming_chat = ChatOpenAI(
            model="gpt-4o-mini", temperature=0.9, streaming=True
        )
        self.agent_type = "SIMPLE_ANSWERER"

    async def answer_streaming(
        self, state: StreamingAgentState
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ - ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨"""
        print("\n>> STREAMING SIMPLE_ANSWERER ì‹œì‘")

        if await self._needs_vector_search(state.original_query):
            simple_results = await self._simple_search(state.original_query)
        else:
            simple_results = []

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        memory_context = getattr(state, "memory_context", "")
        print(
            f"- ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©: {len(memory_context)}ì"
            if memory_context
            else "- ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"
        )

        full_response = ""
        prompt = self._create_enhanced_prompt_with_memory(
            state.original_query, simple_results, memory_context
        )

        async for chunk in self.streaming_chat.astream(prompt):
            if chunk.content:
                full_response += chunk.content
                yield chunk.content

        state.final_answer = full_response
        print(f"\n- ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(full_response)}ì)")

    def _create_enhanced_prompt_with_memory(
        self, query: str, search_results: list, memory_context: str
    ) -> str:
        """ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸"""
        from datetime import datetime

        current_date_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        context_summary = ""
        if search_results:
            summary_parts = []
            for result in search_results[:3]:
                if isinstance(result, dict):
                    content = result.get("content", str(result))
                else:
                    content = getattr(result, "content", str(result))
                summary_parts.append(f"- {content[:300]}...")
            context_summary = "\n".join(summary_parts)

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
        memory_info = ""
        if memory_context:
            memory_info = f"""
**ì´ì „ ëŒ€í™”ì—ì„œ ê¸°ì–µí•´ì•¼ í•  ì •ë³´:**
{memory_context}

ì¤‘ìš”: ìœ„ ì •ë³´ëŠ” ì´ì „ ëŒ€í™”ì—ì„œ ë‚˜ëˆˆ ë‚´ìš©ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì´ë¦„ì„ ì•Œë ¤ì¤¬ë‹¤ë©´ ê·¸ ì´ë¦„ìœ¼ë¡œ ë¶€ë¥´ê³ , ì´ì „ì— ì–¸ê¸‰ëœ ì •ë³´ë“¤ì„ ê¸°ì–µí•˜ê³  ìˆìŒì„ ë³´ì—¬ì£¼ì„¸ìš”.
"""

        return f"""
ë‹¹ì‹ ì€ ë†ìˆ˜ì‚°ë¬¼ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ê°œì¸í™”ëœ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

**ì˜¤ëŠ˜ ë‚ ì§œ:** {current_date_str}

{memory_info}

**í˜„ì¬ ì§ˆë¬¸:** "{query}"

**ê´€ë ¨ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´:**
{context_summary if context_summary else "ê´€ë ¨ ë°ì´í„° ì—†ìŒ"}

**ë‹µë³€ ì§€ì¹¨:**
1. **ì´ì „ ëŒ€í™” í™œìš©:**
   - ì‚¬ìš©ìê°€ ì´ë¦„ì„ ì•Œë ¤ì¤€ ì ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ì´ë¦„ìœ¼ë¡œ ë¶€ë¥´ê¸°
   - ì´ì „ì— ì–¸ê¸‰ëœ ì •ë³´ë“¤ì„ ê¸°ì–µí•˜ê³  ìˆìŒì„ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì—¬ì£¼ê¸°
   - ëŒ€í™”ì˜ ì—°ì†ì„±ì„ ìœ ì§€í•˜ì—¬ ê°œì¸í™”ëœ ë‹µë³€ ì œê³µ

2. **ì§ˆë¬¸ ìœ í˜•ë³„ ë‹µë³€:**
   - ê°œì¸ ì •ë³´ í™•ì¸(ì´ë¦„ ë“±): ì´ì „ ëŒ€í™”ì—ì„œ ì•Œë ¤ì¤€ ì •ë³´ë¥¼ ì •í™•íˆ ë‹µë³€
   - ìºì£¼ì–¼í•œ ì¸ì‚¬: ì¹œê·¼í•˜ê³  ë”°ëœ»í•˜ê²Œ, ê°€ëŠ¥í•˜ë©´ ì´ë¦„ í¬í•¨
   - ì •ë³´ì„± ì§ˆë¬¸: ì² ì €í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€

3. **í†¤ê³¼ ìŠ¤íƒ€ì¼:**
   - ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ëŒ€í™”ì²´
   - ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìˆë‹¤ëŠ” ëŠë‚Œ ì „ë‹¬
   - ë†ìˆ˜ì‚°ë¬¼ ì „ë¬¸ê°€ë¡œì„œì˜ ì‹ ë¢°ì„± ìœ ì§€

4. **ì¶œë ¥ í˜•ì‹ (ì¤‘ìš”):**
   - ëª¨ë“  ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±
   - ì œëª©ì´ í•„ìš”í•œ ê²½ìš°: ## ì œëª©(ì •ë§ ì œëª© ì‘ì„±ì´ í•„ìš”í•œ ê²½ìš°ë§Œ ë‹¨ìˆœ ë‹µë³€ì—ëŠ” ì œëª© ì—†ì´ ë‹µí•´ë„ ë¨)
   - ê°•ì¡°ê°€ í•„ìš”í•œ ë‹¨ì–´: **ê°•ì¡°**
   - ëª©ë¡ì´ í•„ìš”í•œ ê²½ìš°: - í•­ëª©1, - í•­ëª©2
   - ê¸´ ë‹µë³€ì˜ ê²½ìš° ì ì ˆí•œ ë‹¨ë½ êµ¬ë¶„ ì‚¬ìš©
   - í‘œê°€ í•„ìš”í•œ ê²½ìš°: | ì»¬ëŸ¼1 | ì»¬ëŸ¼2 | í˜•íƒœë¡œ ì‘ì„±
   - ìˆ˜ì‹ì€ ê¼­ Latexë¬¸ë²•ìœ¼ë¡œ í‘œí˜„(Reactì—ì„œ ë Œë”ë§ ê°€ëŠ¥í•˜ë„ë¡)
   - ì°¨íŠ¸ ìƒì„± í›„ì—ëŠ” í•´ë‹¹ ì°¨íŠ¸ì— ëŒ€í•œ ì„¤ëª…ê³¼ ì£¼ìš” ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ ('>')ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.(ì˜ˆì‹œ: > ì´ ì°¨íŠ¸ëŠ” ê° ìº í˜ì¸ì˜ ì˜ˆìƒ ROIë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¶”ì • ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê²½ìƒë¶ë„ê°€ ì§‘ì¤‘í•´ì•¼ í•  ìº í˜ì¸ ì „ëµì„ ì‹œì‚¬í•©ë‹ˆë‹¤.)

**ë‹µë³€ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ):**
"""

    def _create_enhanced_prompt(self, query: str, search_results: list) -> str:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ - ë©”ëª¨ë¦¬ ì—†ì´, ë§ˆí¬ë‹¤ìš´ ì§€ì¹¨ í¬í•¨"""
        from datetime import datetime

        current_date_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

        context_summary = ""
        if search_results:
            summary_parts = []
            for result in search_results[:3]:
                if isinstance(result, dict):
                    content = result.get("content", str(result))
                else:
                    content = getattr(result, "content", str(result))
                summary_parts.append(f"- {content[:300]}...")
            context_summary = "\n".join(summary_parts)

        return f"""
ë‹¹ì‹ ì€ ë†ìˆ˜ì‚°ë¬¼ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë„ì›€ì´ ë˜ê³ , ì •í™•í•˜ë©°, ì •ì§í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

**ì˜¤ëŠ˜ ë‚ ì§œ:** {current_date_str}

**ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´:**
{context_summary if context_summary else "ë‚´ë¶€ ìë£Œì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

**ì‚¬ìš©ì ì§ˆë¬¸:** "{query}"

**ë‹µë³€ ì§€ì¹¨:**
1. **ì§ˆë¬¸ ìœ í˜• íŒë‹¨:**
   - ìºì£¼ì–¼í•œ ì¸ì‚¬(ì•ˆë…•, í•˜ì´, ê³ ë§ˆì›Œ ë“±): 1-2ë¬¸ì¥ì˜ ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ë‹µë³€
   - ì •ë³´ ìš”ì²­: ì² ì €í•˜ê³  ì˜ ì •ë¦¬ëœ ë‹µë³€ ì œê³µ

2. **ì •ë³´ì„± ë‹µë³€ì˜ ê²½ìš°:**
   - ê´€ë ¨ì„± ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í™œìš©
   - ë…¼ë¦¬ì  íë¦„ìœ¼ë¡œ ëª…í™•í•˜ê²Œ êµ¬ì¡°í™”
   - ë„ì›€ì´ ë  ë•Œ êµ¬ì²´ì  ì„¸ë¶€ì‚¬í•­ê³¼ ì˜ˆì‹œ í¬í•¨
   - ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ê°€ ë¶ˆì™„ì „í•œ ê²½ìš° í•œê³„ ì¸ì •

3. **ì¶œë ¥ í˜•ì‹ (í•„ìˆ˜!):**
   - ëª¨ë“  ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±
   - ì œëª©: ## ì œëª©, ### ì†Œì œëª©(ì •ë§ í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
   - ê°•ì¡°: **ì¤‘ìš”í•œ ë‚´ìš©**
   - ëª©ë¡: - í•­ëª©1, - í•­ëª©2
   - í‘œ: | ì»¬ëŸ¼1 | ì»¬ëŸ¼2 |
   - ì ì ˆí•œ ë‹¨ë½ êµ¬ë¶„ ì‚¬ìš©
   - ìˆ˜ì‹ì€ Latex ë¬¸ë²•ìœ¼ë¡œ í‘œí˜„

**ë‹µë³€ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ):**
"""

    async def _generate_full_answer(
        self, query: str, search_results: list, memory_context: str = ""
    ) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ì „ì²´ ë‹µë³€ì„ í•œ ë²ˆì— ìƒì„±í•©ë‹ˆë‹¤. - ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨"""
        prompt = self._create_enhanced_prompt_with_memory(
            query, search_results, memory_context
        )
        try:
            response = await self.non_streaming_chat.ainvoke(prompt)
            return response.content
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def _needs_vector_search(self, query: str) -> bool:
        """Vector DB ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ - í–¥ìƒëœ ë¡œì§"""
        prompt = f"""
Analyze the user's question and determine if it requires searching internal knowledge base.

Guidelines:
- Return "YES" for questions asking for specific information, facts, or explanations
- Return "NO" for casual greetings, thanks, personal questions, or general conversation
- Personal questions like "ë‚´ ì´ë¦„ì´ ë­ì•¼?" should return "NO" (use memory instead)

Examples:
- "ì•ˆë…•í•˜ì„¸ìš”" â†’ NO
- "ê³ ë§ˆì›Œìš”" â†’ NO
- "ë‚´ ì´ë¦„ì´ ë­ì•¼?" â†’ NO
- "í€´ë…¸ì•„ì˜ ì˜ì–‘ì„±ë¶„ì´ ê¶ê¸ˆí•´ìš”" â†’ YES
- "ê°€ê²© ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”" â†’ YES

Question: "{query}"
Decision (YES/NO):
"""
        try:
            response = await self.non_streaming_chat.ainvoke(prompt)
            decision = "yes" in response.content.lower()
            print(f"Vector DB ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            print(f"- ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨: {'í•„ìš”' if decision else 'ë¶ˆí•„ìš”'}")
            return decision
        except Exception:
            return True  # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ê²€ìƒ‰ ìˆ˜í–‰

    async def _simple_search(self, query: str):
        """Vector DB ê°„ë‹¨ ê²€ìƒ‰ - í–¥ìƒëœ ê²°ê³¼ ì²˜ë¦¬"""
        try:
            if self.vector_db:
                vector_results = self.vector_db.search(query)

                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
                search_results = []
                for doc in vector_results[:5]:  # ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                    search_results.append(
                        {
                            "content": doc.get("content", ""),
                            "source": "vector_db",
                            "relevance_score": doc.get("similarity_score", 0.7),
                            "title": doc.get("title", "Unknown"),
                        }
                    )

                print(f"- Vector DB ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
                return search_results
            else:
                print("- Vector DBê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                return []
        except Exception as e:
            print(f"- Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []


# CriticAgent2: ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ/ì‹ ë¢°ë„ í‰ê°€
class CriticAgent2:
    def __init__(self):
        self.chat = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.agent_type = AgentType.CRITIC_2

    async def evaluate(self, state: StreamingAgentState) -> StreamingAgentState:
        print(">> CRITIC_2 ì‹œì‘")
        integrated_context = state.integrated_context
        original_query = state.original_query

        if not integrated_context:
            state.critic2_result = CriticResult(
                status="insufficient",
                suggestion="í†µí•©ëœ ë§¥ë½ì´ ì—†ì–´ í‰ê°€ ë¶ˆê°€",
                confidence=0.0,
                reasoning="ë§¥ë½ í†µí•© ë‹¨ê³„ ë¯¸ì™„ë£Œ",
            )
            state.context_sufficient = False
            return state

        print(f"- í†µí•© ë§¥ë½ ê¸¸ì´: {len(integrated_context)}ì")

        evaluation_result = await self._evaluate_context_quality(
            original_query, integrated_context, state.critic1_result
        )

        print(f"- í‰ê°€ ê²°ê³¼: {evaluation_result.get('status', 'insufficient')}")
        print(f"- í‰ê°€ ì´ìœ : {evaluation_result.get('reasoning', 'N/A')}")

        state.critic2_result = CriticResult(**evaluation_result)

        if evaluation_result.get("status") == "sufficient":
            state.context_sufficient = True
            print("- ë§¥ë½ ì™„ì„±ë„ ì¶©ë¶„ - ë³´ê³ ì„œ ìƒì„± ê°€ëŠ¥")
        else:
            state.context_sufficient = False
            print("- ë§¥ë½ ì™„ì„±ë„ ë¶€ì¡± - ì¶”ê°€ ë³´ì™„ í•„ìš”")

        memory = state.get_agent_memory(AgentType.CRITIC_2)
        memory.add_finding(f"ë§¥ë½ ì™„ì„±ë„ í‰ê°€: {state.context_sufficient}")
        memory.update_metric(
            "context_quality_score", evaluation_result.get("confidence", 0.5)
        )
        print("\n>> CRITIC_2 ì™„ë£Œ")
        return state

    async def _evaluate_context_quality(
        self, original_query, integrated_context, critic1_result
    ):
        critic1_summary = "ì´ì „ Critic1ì˜ í”¼ë“œë°± ì—†ìŒ. (1ì°¨ ê²€ìˆ˜ í†µê³¼)"
        if critic1_result and critic1_result.status == "insufficient":
            critic1_summary = f"ì´ì „ ë‹¨ê³„ì—ì„œ ì •ë³´ ë¶€ì¡± í‰ê°€ê°€ ìˆì—ˆìŒ. (í”¼ë“œë°±: '{critic1_result.suggestion}')"

        prompt = f"""
        ë‹¹ì‹ ì€ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±ì„ ì•ë‘ê³ , í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ ë° í†µí•©ëœ ì •ë³´ê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì´ ë  ìˆ˜ ìˆëŠ”ì§€ ìµœì¢… ê²€ìˆ˜í•˜ëŠ” ìˆ˜ì„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

        ### ìµœì¢… ê²€ìˆ˜ ê¸°ì¤€:
        1.  **ë‹µë³€ì˜ ì™„ì„±ë„:** 'í†µí•©ëœ ë§¥ë½'ì´ 'ì›ë³¸ ì§ˆë¬¸'ì— ëŒ€í•´ ì™„ì „í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ê°€? ëª¨í˜¸í•˜ê±°ë‚˜ ë¹ ì§„ ë¶€ë¶„ì€ ì—†ëŠ”ê°€?
        2.  **ë…¼ë¦¬ì  íë¦„:** ì •ë³´ë“¤ì´ ìì—°ìŠ¤ëŸ½ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆëŠ”ê°€? ì´ì•¼ê¸°ì˜ íë¦„ì´ ë§¤ë„ëŸ¬ìš´ê°€?
        3.  **í”¼ë“œë°± ë°˜ì˜ ì—¬ë¶€:** (ë§Œì•½ ìˆë‹¤ë©´) 'ì´ì „ ë‹¨ê³„ í”¼ë“œë°±'ì—ì„œ ìš”êµ¬í•œ ë‚´ìš©ì´ 'í†µí•©ëœ ë§¥ë½'ì— ì˜ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?
        ---

        <ì›ë³¸ ì§ˆë¬¸>
        "{original_query}"

        <ì´ì „ ë‹¨ê³„ í”¼ë“œë°±>
        {critic1_summary}

        <ìµœì¢… ë³´ê³ ì„œì˜ ê¸°ë°˜ì´ ë  í†µí•©ëœ ë§¥ë½>
        {integrated_context}
        ---

        **[ìµœì¢… ê²€ìˆ˜ ê²°ê³¼]** (ì•„ë˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.)
        STATUS: sufficient ë˜ëŠ” insufficient
        REASONING: [íŒë‹¨ ê·¼ê±°ë¥¼ ê°„ê²°í•˜ê²Œ ì‘ì„±. í”¼ë“œë°±ì´ ì˜ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰.]
        SUGGESTION: [insufficientì¼ ê²½ìš°, ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì „ì— ë¬´ì—‡ì„ ë” ë³´ê°•í•´ì•¼ í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆ.]
        CONFIDENCE: [0.0 ~ 1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„ ì ìˆ˜]
        """
        response = await self.chat.ainvoke(prompt)
        return self._parse_evaluation(response.content)

    def _parse_evaluation(self, response_content):
        # Critic1ê³¼ ë™ì¼í•œ íŒŒì„œë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©
        try:
            lines = response_content.strip().split("\n")
            result = {}
            for line in lines:
                if line.startswith("STATUS:"):
                    result["status"] = line.split(":", 1)[1].strip()
                elif line.startswith("REASONING:"):
                    result["reasoning"] = line.split(":", 1)[1].strip()
                elif line.startswith("SUGGESTION:"):
                    result["suggestion"] = line.split(":", 1)[1].strip()
                elif line.startswith("CONFIDENCE:"):
                    try:
                        result["confidence"] = float(line.split(":", 1)[1].strip())
                    except:
                        result["confidence"] = 0.5

            if "status" not in result:
                result["status"] = "insufficient"
            if "reasoning" not in result:
                result["reasoning"] = "íŒë‹¨ ê·¼ê±° ì—†ìŒ"
            if result.get("status") == "insufficient" and not result.get("suggestion"):
                result["suggestion"] = "ë‚´ìš© ë³´ê°•ì´ í•„ìš”í•©ë‹ˆë‹¤."

            return result
        except Exception as e:
            print(f"- íŒŒì‹± ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return {
                "status": "insufficient",
                "reasoning": "í‰ê°€ íŒŒì‹± ì‹¤íŒ¨",
                "suggestion": "ë§¥ë½ ì¬êµ¬ì„± ê¶Œì¥",
                "confidence": 0.5,
            }
